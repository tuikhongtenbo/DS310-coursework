{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14377126,"sourceType":"datasetVersion","datasetId":9181544}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -e . -q\n!pip install -q datasets evaluate\n!pip install wandb weave","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:39:06.144665Z","iopub.execute_input":"2026-01-05T15:39:06.145313Z","iopub.status.idle":"2026-01-05T15:39:13.980114Z","shell.execute_reply.started":"2026-01-05T15:39:06.145283Z","shell.execute_reply":"2026-01-05T15:39:13.979384Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: file:///kaggle/working/DS310/TH3 does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\nRequirement already satisfied: weave in /usr/local/lib/python3.12/dist-packages (0.52.22)\nRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.5)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\nRequirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.12/dist-packages (from weave) (5.6.3)\nRequirement already satisfied: gql>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from gql[httpx]>=3.0.0->weave) (4.0.0)\nRequirement already satisfied: jsonschema>=4.23.0 in /usr/local/lib/python3.12/dist-packages (from weave) (4.25.1)\nRequirement already satisfied: polyfile-weave in /usr/local/lib/python3.12/dist-packages (from weave) (0.5.7)\nRequirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/local/lib/python3.12/dist-packages (from weave) (8.5.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: graphql-core<3.3,>=3.2 in /usr/local/lib/python3.12/dist-packages (from gql>=3.0.0->gql[httpx]>=3.0.0->weave) (3.2.7)\nRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.12/dist-packages (from gql>=3.0.0->gql[httpx]>=3.0.0->weave) (1.22.0)\nRequirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from gql>=3.0.0->gql[httpx]>=3.0.0->weave) (2.2.1)\nRequirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gql>=3.0.0->gql[httpx]>=3.0.0->weave) (4.12.0)\nRequirement already satisfied: httpx<1,>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from gql[httpx]>=3.0.0->weave) (0.28.1)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (25.4.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (0.27.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\nRequirement already satisfied: abnf~=2.2.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (2.2.0)\nRequirement already satisfied: chardet>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (5.2.0)\nRequirement already satisfied: cint>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (1.0.0)\nRequirement already satisfied: fickling>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (0.1.6)\nRequirement already satisfied: graphviz>=0.20.1 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (0.21)\nRequirement already satisfied: intervaltree>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (3.2.1)\nRequirement already satisfied: jinja2>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (3.1.6)\nRequirement already satisfied: kaitaistruct~=0.10 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (0.11)\nRequirement already satisfied: networkx>=2.6.3 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (3.5)\nRequirement already satisfied: pdfminer.six<=20250506,>=20220524 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (20250506)\nRequirement already satisfied: Pillow>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (11.3.0)\nRequirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (80.9.0)\nRequirement already satisfied: stdlib-list~=0.11.1 in /usr/local/lib/python3.12/dist-packages (from fickling>=0.0.8->polyfile-weave->weave) (0.11.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->gql[httpx]>=3.0.0->weave) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.27.0->gql[httpx]>=3.0.0->weave) (0.16.0)\nRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from intervaltree>=2.4.0->polyfile-weave->weave) (2.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.1.0->polyfile-weave->weave) (3.0.3)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (46.0.3)\nRequirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql>=3.0.0->gql[httpx]>=3.0.0->weave) (6.7.0)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql>=3.0.0->gql[httpx]>=3.0.0->weave) (0.4.1)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (2.23)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!git clone https://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@github.com/tuikhongtenbo/DS310","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:55:20.084289Z","iopub.execute_input":"2026-01-05T15:55:20.085119Z","iopub.status.idle":"2026-01-05T15:55:22.108226Z","shell.execute_reply.started":"2026-01-05T15:55:20.085085Z","shell.execute_reply":"2026-01-05T15:55:22.107462Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'DS310'...\nremote: Enumerating objects: 198, done.\u001b[K\nremote: Counting objects: 100% (198/198), done.\u001b[K\nremote: Compressing objects: 100% (140/140), done.\u001b[K\nremote: Total 198 (delta 107), reused 142 (delta 54), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (198/198), 22.23 MiB | 24.72 MiB/s, done.\nResolving deltas: 100% (107/107), done.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"%cd /kaggle/working/DS310/TH3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:55:22.109693Z","iopub.execute_input":"2026-01-05T15:55:22.109954Z","iopub.status.idle":"2026-01-05T15:55:22.115067Z","shell.execute_reply.started":"2026-01-05T15:55:22.109927Z","shell.execute_reply":"2026-01-05T15:55:22.114405Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/DS310/TH3\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"DS310-ViQuAD2-QA\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:39:16.081988Z","iopub.execute_input":"2026-01-05T15:39:16.082237Z","iopub.status.idle":"2026-01-05T15:39:16.191999Z","shell.execute_reply.started":"2026-01-05T15:39:16.082210Z","shell.execute_reply":"2026-01-05T15:39:16.191391Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"MODEL_NAME=\"vinai/phobert-base\"\nOUTPUT_DIR=\"/kaggle/working/phobert-base\"\n\n!python run_qa.py \\\n  --model_name_or_path $MODEL_NAME \\\n  --train_file \"/kaggle/input/uit-viquad-2-0/train.json\" \\\n  --validation_file \"/kaggle/input/uit-viquad-2-0/dev.json\" \\\n  --do_train \\\n  --do_eval \\\n  --eval_strategy steps \\\n  --save_strategy steps \\\n  --per_device_train_batch_size 32 \\\n  --per_device_eval_batch_size 12 \\\n  --gradient_accumulation_steps 5 \\\n  --learning_rate 1e-4 \\\n  --num_train_epochs 2 \\\n  --warmup_ratio 0.1 \\\n  --max_seq_length 256 \\\n  --doc_stride 128 \\\n  --fp16 \\\n  --dataloader_num_workers 4 \\\n  --logging_steps 100 \\\n  --save_steps 500 \\\n  --eval_steps 500 \\\n  --load_best_model_at_end \\\n  --metric_for_best_model f1 \\\n  --greater_is_better true \\\n  --save_total_limit 3 \\\n  --seed 53 \\\n  --weight_decay 0.01 \\\n  --adam_epsilon 1e-8 \\\n  --max_grad_norm 1.0 \\\n  --output_dir $OUTPUT_DIR \\\n  --version_2_with_negative \\\n  --n_best_size 20 \\\n  --max_answer_length 30 \\\n  --null_score_diff_threshold 0.0 \\\n  --overwrite_output_dir \\\n  --report_to wandb \\\n  --run_name \"phobert-base-2epoch-bs32\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:55:23.829040Z","iopub.execute_input":"2026-01-05T15:55:23.829592Z","iopub.status.idle":"2026-01-05T16:21:04.200814Z","shell.execute_reply.started":"2026-01-05T15:55:23.829563Z","shell.execute_reply":"2026-01-05T16:21:04.200073Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2026-01-05 15:55:28.488407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767628528.510227     415 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767628528.517426     415 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767628528.534781     415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767628528.534819     415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767628528.534823     415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767628528.534827     415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nWARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\nINFO:__main__:Training/evaluation parameters TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=True,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=4,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=500,\neval_strategy=IntervalStrategy.STEPS,\neval_use_gather_object=False,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=5,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=None,\nhub_revision=None,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=no,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0001,\nlength_column_name=length,\nliger_kernel_config=None,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=/kaggle/working/phobert-base/runs/Jan05_15-55-34_69d344501b91,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=100,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_kwargs={},\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=f1,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=2.0,\noptim=OptimizerNames.ADAMW_TORCH_FUSED,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=/kaggle/working/phobert-base,\noverwrite_output_dir=True,\nparallelism_config=None,\npast_index=-1,\nper_device_eval_batch_size=12,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\nproject=huggingface,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=phobert-base-2epoch-bs32,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=SaveStrategy.STEPS,\nsave_total_limit=3,\nseed=53,\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\ntrackio_space_id=trackio,\nuse_cpu=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\n)\nUsing custom data configuration default-428800dcce561d93\nINFO:datasets.builder:Using custom data configuration default-428800dcce561d93\nFound cached dataset json (/root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nINFO:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\n[INFO|configuration_utils.py:765] 2026-01-05 15:55:35,028 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 15:55:35,029 >> Model config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 258,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"tokenizer_class\": \"PhobertTokenizer\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 64001\n}\n\n[INFO|tokenization_auto.py:922] 2026-01-05 15:55:35,114 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n[INFO|configuration_utils.py:765] 2026-01-05 15:55:35,210 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 15:55:35,210 >> Model config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 258,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"tokenizer_class\": \"PhobertTokenizer\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 64001\n}\n\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:35,521 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/vocab.txt\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:35,521 >> loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/bpe.codes\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:35,521 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:35,521 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:35,521 >> loading file tokenizer_config.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:35,521 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/tokenizer.json\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:35,521 >> loading file chat_template.jinja from cache at None\n[INFO|configuration_utils.py:765] 2026-01-05 15:55:35,522 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 15:55:35,523 >> Model config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 258,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"tokenizer_class\": \"PhobertTokenizer\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 64001\n}\n\nWARNING:__main__:PhoBERT does not provide a native *Fast* tokenizer in this Transformers version. Falling back to RobertaTokenizerFast so we can use offsets_mapping.\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:36,181 >> loading file vocab.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:36,181 >> loading file merges.txt from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:36,181 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/tokenizer.json\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:36,181 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:36,181 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:36,181 >> loading file tokenizer_config.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 15:55:36,181 >> loading file chat_template.jinja from cache at None\n[INFO|configuration_utils.py:765] 2026-01-05 15:55:36,182 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 15:55:36,183 >> Model config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 258,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"tokenizer_class\": \"PhobertTokenizer\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 64001\n}\n\n[WARNING|tokenization_utils_base.py:2237] 2026-01-05 15:55:36,183 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'PhobertTokenizer'. \nThe class this function is called from is 'RobertaTokenizerFast'.\nWARNING:__main__:Tokenizer vocab size (66119) is larger than model vocab size (64001). This may cause token ID out of range errors. Consider using the original slow tokenizer or ensuring tokenizer vocab matches model vocab.\n[INFO|modeling_utils.py:1172] 2026-01-05 15:55:36,526 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/c1e37c5c86f918761049cef6fa216b4779d0d01d/pytorch_model.bin\n[INFO|safetensors_conversion.py:61] 2026-01-05 15:55:36,917 >> Attempting to create safetensors variant\n[INFO|modeling_utils.py:5525] 2026-01-05 15:55:37,124 >> Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:5535] 2026-01-05 15:55:37,124 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nRunning tokenizer on train dataset:   0%|      | 0/22765 [00:00<?, ? examples/s][INFO|safetensors_conversion.py:74] 2026-01-05 15:55:37,284 >> Safetensors PR exists\nWARNING:__main__:Clipped token IDs in 37 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nCaching processed dataset at /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-45330e48563c8a73.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-45330e48563c8a73.arrow\nRunning tokenizer on train dataset:   4%| | 1000/22765 [00:00<00:15, 1426.09 exaWARNING:__main__:Clipped token IDs in 21 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:   9%| | 2000/22765 [00:01<00:14, 1481.92 exaWARNING:__main__:Clipped token IDs in 22 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  13%|‚ñè| 3000/22765 [00:02<00:13, 1502.14 exaWARNING:__main__:Clipped token IDs in 26 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  18%|‚ñè| 4000/22765 [00:02<00:12, 1501.25 exaWARNING:__main__:Clipped token IDs in 17 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  22%|‚ñè| 5000/22765 [00:03<00:11, 1491.78 exaWARNING:__main__:Clipped token IDs in 21 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  26%|‚ñé| 6000/22765 [00:04<00:11, 1486.05 exaWARNING:__main__:Clipped token IDs in 15 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  31%|‚ñé| 7000/22765 [00:05<00:12, 1268.73 exaWARNING:__main__:Clipped token IDs in 27 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  35%|‚ñé| 8000/22765 [00:05<00:11, 1336.26 exaWARNING:__main__:Clipped token IDs in 27 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  40%|‚ñç| 9000/22765 [00:06<00:10, 1368.16 exaWARNING:__main__:Clipped token IDs in 16 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  44%|‚ñç| 10000/22765 [00:07<00:09, 1401.71 exWARNING:__main__:Clipped token IDs in 24 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  48%|‚ñç| 11000/22765 [00:07<00:08, 1423.91 exWARNING:__main__:Clipped token IDs in 24 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  53%|‚ñå| 12000/22765 [00:08<00:07, 1438.03 exWARNING:__main__:Clipped token IDs in 17 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  57%|‚ñå| 13000/22765 [00:09<00:06, 1448.10 exWARNING:__main__:Clipped token IDs in 23 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  61%|‚ñå| 14000/22765 [00:09<00:06, 1451.36 exWARNING:__main__:Clipped token IDs in 29 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  66%|‚ñã| 15000/22765 [00:10<00:05, 1456.20 exWARNING:__main__:Clipped token IDs in 24 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  70%|‚ñã| 16000/22765 [00:11<00:04, 1474.85 exWARNING:__main__:Clipped token IDs in 23 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  75%|‚ñã| 17000/22765 [00:11<00:03, 1484.62 exWARNING:__main__:Clipped token IDs in 27 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  79%|‚ñä| 18000/22765 [00:12<00:03, 1493.21 exWARNING:__main__:Clipped token IDs in 32 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  83%|‚ñä| 19000/22765 [00:13<00:02, 1506.74 exWARNING:__main__:Clipped token IDs in 18 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  88%|‚ñâ| 20000/22765 [00:13<00:01, 1491.78 exWARNING:__main__:Clipped token IDs in 16 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  92%|‚ñâ| 21000/22765 [00:14<00:01, 1476.25 exWARNING:__main__:Clipped token IDs in 22 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset:  97%|‚ñâ| 22000/22765 [00:15<00:00, 1484.69 exWARNING:__main__:Clipped token IDs in 19 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on train dataset: 100%|‚ñà| 22765/22765 [00:15<00:00, 1446.40 ex\nRunning tokenizer on validation dataset:   0%|  | 0/5692 [00:00<?, ? examples/s]WARNING:__main__:Clipped token IDs in 16 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nCaching processed dataset at /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-de9fb04708b94ab8.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-de9fb04708b94ab8.arrow\nRunning tokenizer on validation dataset:  18%|‚ñè| 1000/5692 [00:00<00:03, 1178.52WARNING:__main__:Clipped token IDs in 25 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on validation dataset:  35%|‚ñé| 2000/5692 [00:01<00:03, 1186.81WARNING:__main__:Clipped token IDs in 14 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on validation dataset:  53%|‚ñå| 3000/5692 [00:02<00:02, 975.10 WARNING:__main__:Clipped token IDs in 26 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on validation dataset:  70%|‚ñã| 4000/5692 [00:03<00:01, 1034.09WARNING:__main__:Clipped token IDs in 25 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on validation dataset:  88%|‚ñâ| 5000/5692 [00:04<00:00, 1084.40WARNING:__main__:Clipped token IDs in 17 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on validation dataset: 100%|‚ñà| 5692/5692 [00:05<00:00, 1076.71\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n[INFO|trainer.py:749] 2026-01-05 15:56:00,449 >> Using auto half precision backend\n[INFO|trainer.py:2519] 2026-01-05 15:56:00,843 >> ***** Running training *****\n[INFO|trainer.py:2520] 2026-01-05 15:56:00,843 >>   Num examples = 32,454\n[INFO|trainer.py:2521] 2026-01-05 15:56:00,843 >>   Num Epochs = 2\n[INFO|trainer.py:2522] 2026-01-05 15:56:00,843 >>   Instantaneous batch size per device = 32\n[INFO|trainer.py:2525] 2026-01-05 15:56:00,843 >>   Total train batch size (w. parallel, distributed & accumulation) = 160\n[INFO|trainer.py:2526] 2026-01-05 15:56:00,843 >>   Gradient Accumulation steps = 5\n[INFO|trainer.py:2527] 2026-01-05 15:56:00,843 >>   Total optimization steps = 406\n[INFO|trainer.py:2528] 2026-01-05 15:56:00,843 >>   Number of trainable parameters = 134,409,218\n[INFO|integration_utils.py:867] 2026-01-05 15:56:00,844 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mply58509\u001b[0m (\u001b[33mply58509-uit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/DS310/TH3/wandb/run-20260105_155601-83f86it1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mphobert-base-2epoch-bs32\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ply58509-uit/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ply58509-uit/huggingface/runs/83f86it1\u001b[0m\n{'loss': 3.1451, 'grad_norm': 4.277865886688232, 'learning_rate': 8.410958904109589e-05, 'epoch': 0.49}\n{'loss': 1.5429, 'grad_norm': 5.334636688232422, 'learning_rate': 5.671232876712329e-05, 'epoch': 0.99}\n{'loss': 1.2278, 'grad_norm': 4.348659992218018, 'learning_rate': 2.931506849315069e-05, 'epoch': 1.48}\n{'loss': 1.1486, 'grad_norm': 4.933060169219971, 'learning_rate': 1.9178082191780823e-06, 'epoch': 1.97}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 406/406 [23:28<00:00,  3.32s/it][INFO|trainer.py:4309] 2026-01-05 16:19:31,205 >> Saving model checkpoint to /kaggle/working/phobert-base/checkpoint-406\n[INFO|configuration_utils.py:491] 2026-01-05 16:19:31,207 >> Configuration saved in /kaggle/working/phobert-base/checkpoint-406/config.json\n[INFO|modeling_utils.py:4181] 2026-01-05 16:19:32,204 >> Model weights saved in /kaggle/working/phobert-base/checkpoint-406/model.safetensors\n[INFO|tokenization_utils_base.py:2590] 2026-01-05 16:19:32,206 >> tokenizer config file saved in /kaggle/working/phobert-base/checkpoint-406/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2599] 2026-01-05 16:19:32,207 >> Special tokens file saved in /kaggle/working/phobert-base/checkpoint-406/special_tokens_map.json\n[INFO|trainer.py:2810] 2026-01-05 16:19:33,650 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 1412.8072, 'train_samples_per_second': 45.943, 'train_steps_per_second': 0.287, 'train_loss': 1.7570325271249405, 'epoch': 2.0}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 406/406 [23:31<00:00,  3.48s/it]\n[INFO|trainer.py:4309] 2026-01-05 16:19:33,658 >> Saving model checkpoint to /kaggle/working/phobert-base\n[INFO|configuration_utils.py:491] 2026-01-05 16:19:33,660 >> Configuration saved in /kaggle/working/phobert-base/config.json\n[INFO|modeling_utils.py:4181] 2026-01-05 16:19:34,596 >> Model weights saved in /kaggle/working/phobert-base/model.safetensors\n[INFO|tokenization_utils_base.py:2590] 2026-01-05 16:19:34,598 >> tokenizer config file saved in /kaggle/working/phobert-base/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2599] 2026-01-05 16:19:34,599 >> Special tokens file saved in /kaggle/working/phobert-base/special_tokens_map.json\n***** train metrics *****\n  epoch                    =        2.0\n  total_flos               =  7897731GF\n  train_loss               =      1.757\n  train_runtime            = 0:23:32.80\n  train_samples            =      32454\n  train_samples_per_second =     45.943\n  train_steps_per_second   =      0.287\nINFO:__main__:*** Evaluate ***\n[INFO|trainer.py:1012] 2026-01-05 16:19:34,673 >> The following columns in the Evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4643] 2026-01-05 16:19:34,678 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4645] 2026-01-05 16:19:34,678 >>   Num examples = 8027\n[INFO|trainer.py:4648] 2026-01-05 16:19:34,679 >>   Batch size = 12\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [01:01<00:00, 10.11it/s]INFO:utils_qa:Post-processing 5692 example predictions split into 8027 features.\n\n  0%|                                                  | 0/5692 [00:00<?, ?it/s]\u001b[A\n  1%|‚ñé                                       | 38/5692 [00:00<00:15, 372.67it/s]\u001b[A\n  1%|‚ñå                                       | 76/5692 [00:00<00:16, 335.00it/s]\u001b[A\n  2%|‚ñä                                      | 110/5692 [00:00<00:16, 334.86it/s]\u001b[A\n  3%|‚ñâ                                      | 144/5692 [00:00<00:17, 324.67it/s]\u001b[A\n  3%|‚ñà‚ñè                                     | 177/5692 [00:00<00:17, 314.39it/s]\u001b[A\n  4%|‚ñà‚ñç                                     | 209/5692 [00:00<00:17, 314.98it/s]\u001b[A\n  4%|‚ñà‚ñã                                     | 242/5692 [00:00<00:17, 316.44it/s]\u001b[A\n  5%|‚ñà‚ñâ                                     | 274/5692 [00:00<00:18, 293.60it/s]\u001b[A\n  5%|‚ñà‚ñà                                     | 304/5692 [00:00<00:18, 290.68it/s]\u001b[A\n  6%|‚ñà‚ñà‚ñé                                    | 336/5692 [00:01<00:17, 297.74it/s]\u001b[A\n  6%|‚ñà‚ñà‚ñå                                    | 366/5692 [00:01<00:17, 296.87it/s]\u001b[A\n  7%|‚ñà‚ñà‚ñã                                    | 396/5692 [00:01<00:17, 296.79it/s]\u001b[A\n  8%|‚ñà‚ñà‚ñâ                                    | 430/5692 [00:01<00:17, 304.39it/s]\u001b[A\n  8%|‚ñà‚ñà‚ñà‚ñè                                   | 464/5692 [00:01<00:16, 313.96it/s]\u001b[A\n  9%|‚ñà‚ñà‚ñà‚ñç                                   | 496/5692 [00:01<00:17, 291.44it/s]\u001b[A\n  9%|‚ñà‚ñà‚ñà‚ñã                                   | 531/5692 [00:01<00:16, 307.11it/s]\u001b[A\n 10%|‚ñà‚ñà‚ñà‚ñä                                   | 563/5692 [00:01<00:16, 309.17it/s]\u001b[A\n 10%|‚ñà‚ñà‚ñà‚ñà                                   | 597/5692 [00:01<00:16, 316.40it/s]\u001b[A\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 636/5692 [00:02<00:15, 336.23it/s]\u001b[A\n 12%|‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 674/5692 [00:02<00:14, 348.45it/s]\u001b[A\n 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 710/5692 [00:02<00:14, 348.27it/s]\u001b[A\n 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 746/5692 [00:02<00:14, 348.19it/s]\u001b[A\n 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 781/5692 [00:02<00:14, 344.00it/s]\u001b[A\n 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 817/5692 [00:02<00:14, 347.96it/s]\u001b[A\n 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 853/5692 [00:02<00:13, 347.91it/s]\u001b[A\n 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 889/5692 [00:02<00:13, 350.02it/s]\u001b[A\n 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 925/5692 [00:02<00:13, 349.44it/s]\u001b[A\n 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 962/5692 [00:02<00:13, 353.61it/s]\u001b[A\n 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 1001/5692 [00:03<00:12, 363.74it/s]\u001b[A\n 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 1040/5692 [00:03<00:12, 369.32it/s]\u001b[A\n 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 1081/5692 [00:03<00:12, 380.41it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 1120/5692 [00:03<00:12, 372.05it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 1158/5692 [00:03<00:12, 367.83it/s]\u001b[A\n 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 1195/5692 [00:03<00:12, 353.73it/s]\u001b[A\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 1231/5692 [00:03<00:12, 355.14it/s]\u001b[A\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 1267/5692 [00:03<00:12, 350.69it/s]\u001b[A\n 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 1303/5692 [00:03<00:12, 347.73it/s]\u001b[A\n 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1341/5692 [00:03<00:12, 356.12it/s]\u001b[A\n 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1377/5692 [00:04<00:12, 346.75it/s]\u001b[A\n 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1412/5692 [00:04<00:12, 337.02it/s]\u001b[A\n 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1448/5692 [00:04<00:12, 342.24it/s]\u001b[A\n 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1486/5692 [00:04<00:11, 352.14it/s]\u001b[A\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1522/5692 [00:04<00:11, 353.20it/s]\u001b[A\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1558/5692 [00:04<00:11, 354.48it/s]\u001b[A\n 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1597/5692 [00:04<00:11, 361.55it/s]\u001b[A\n 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1637/5692 [00:04<00:10, 372.15it/s]\u001b[A\n 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1675/5692 [00:04<00:11, 362.74it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1712/5692 [00:05<00:11, 354.27it/s]\u001b[A\n 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1751/5692 [00:05<00:10, 364.19it/s]\u001b[A\n 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1788/5692 [00:05<00:11, 353.82it/s]\u001b[A\n 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1824/5692 [00:05<00:11, 349.73it/s]\u001b[A\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1860/5692 [00:05<00:11, 346.79it/s]\u001b[A\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1895/5692 [00:05<00:11, 326.27it/s]\u001b[A\n 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1929/5692 [00:05<00:11, 330.07it/s]\u001b[A\n 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1963/5692 [00:05<00:11, 330.36it/s]\u001b[A\n 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 2000/5692 [00:05<00:10, 341.71it/s]\u001b[A\n 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 2035/5692 [00:06<00:11, 328.70it/s]\u001b[A\n 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 2069/5692 [00:06<00:10, 330.80it/s]\u001b[A\n 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 2103/5692 [00:06<00:11, 310.42it/s]\u001b[A\n 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 2135/5692 [00:06<00:11, 309.16it/s]\u001b[A\n 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 2167/5692 [00:06<00:12, 293.55it/s]\u001b[A\n 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 2199/5692 [00:06<00:11, 296.49it/s]\u001b[A\n 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 2234/5692 [00:06<00:11, 310.57it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 2270/5692 [00:06<00:10, 320.95it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 2303/5692 [00:06<00:10, 313.98it/s]\u001b[A\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 2338/5692 [00:06<00:10, 324.08it/s]\u001b[A\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2371/5692 [00:07<00:10, 315.37it/s]\u001b[A\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 2404/5692 [00:07<00:10, 317.91it/s]\u001b[A\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2444/5692 [00:07<00:09, 338.60it/s]\u001b[A\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 2479/5692 [00:07<00:09, 337.86it/s]\u001b[A\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 2513/5692 [00:07<00:09, 337.36it/s]\u001b[A\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 2551/5692 [00:07<00:09, 345.90it/s]\u001b[A\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 2588/5692 [00:07<00:08, 350.89it/s]\u001b[A\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2624/5692 [00:07<00:08, 348.69it/s]\u001b[A\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2659/5692 [00:07<00:08, 348.20it/s]\u001b[A\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 2694/5692 [00:08<00:08, 348.60it/s]\u001b[A\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2730/5692 [00:08<00:08, 350.06it/s]\u001b[A\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2766/5692 [00:08<00:08, 351.96it/s]\u001b[A\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2805/5692 [00:08<00:07, 362.48it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2842/5692 [00:08<00:08, 355.23it/s]\u001b[A\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2881/5692 [00:08<00:07, 364.25it/s]\u001b[A\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2922/5692 [00:08<00:07, 377.16it/s]\u001b[A\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2960/5692 [00:08<00:07, 367.86it/s]\u001b[A\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2997/5692 [00:08<00:07, 356.74it/s]\u001b[A\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 3033/5692 [00:08<00:07, 356.65it/s]\u001b[A\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 3069/5692 [00:09<00:07, 354.73it/s]\u001b[A\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 3105/5692 [00:09<00:07, 352.24it/s]\u001b[A\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 3146/5692 [00:09<00:06, 367.59it/s]\u001b[A\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 3183/5692 [00:09<00:07, 355.76it/s]\u001b[A\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 3219/5692 [00:09<00:07, 350.82it/s]\u001b[A\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 3255/5692 [00:09<00:07, 338.52it/s]\u001b[A\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 3291/5692 [00:09<00:06, 344.16it/s]\u001b[A\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 3326/5692 [00:09<00:07, 337.50it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [01:17<00:00, 10.11it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 3394/5692 [00:10<00:07, 302.56it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 3428/5692 [00:10<00:07, 312.65it/s]\u001b[A\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 3460/5692 [00:10<00:07, 312.80it/s]\u001b[A\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 3492/5692 [00:10<00:07, 311.43it/s]\u001b[A\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 3526/5692 [00:10<00:06, 317.88it/s]\u001b[A\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 3562/5692 [00:10<00:06, 328.61it/s]\u001b[A\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 3598/5692 [00:10<00:06, 334.26it/s]\u001b[A\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 3632/5692 [00:10<00:06, 307.96it/s]\u001b[A\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 3665/5692 [00:10<00:06, 309.17it/s]\u001b[A\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 3701/5692 [00:11<00:06, 321.70it/s]\u001b[A\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 3737/5692 [00:11<00:05, 329.73it/s]\u001b[A\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 3776/5692 [00:11<00:05, 346.49it/s]\u001b[A\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 3811/5692 [00:11<00:05, 337.74it/s]\u001b[A\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 3848/5692 [00:11<00:05, 345.77it/s]\u001b[A\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3889/5692 [00:11<00:04, 361.90it/s]\u001b[A\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 3926/5692 [00:11<00:04, 364.04it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 3964/5692 [00:11<00:04, 367.93it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 4001/5692 [00:11<00:04, 366.29it/s]\u001b[A\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 4040/5692 [00:11<00:04, 373.02it/s]\u001b[A\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 4080/5692 [00:12<00:04, 377.40it/s]\u001b[A\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 4118/5692 [00:12<00:04, 367.98it/s]\u001b[A\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 4155/5692 [00:12<00:04, 361.68it/s]\u001b[A\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 4192/5692 [00:12<00:04, 344.65it/s]\u001b[A\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 4227/5692 [00:12<00:04, 345.46it/s]\u001b[A\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 4263/5692 [00:12<00:04, 348.17it/s]\u001b[A\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 4302/5692 [00:12<00:03, 358.90it/s]\u001b[A\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 4339/5692 [00:12<00:03, 360.44it/s]\u001b[A\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 4376/5692 [00:12<00:03, 332.32it/s]\u001b[A\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 4411/5692 [00:13<00:03, 335.28it/s]\u001b[A\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 4445/5692 [00:13<00:03, 329.13it/s]\u001b[A\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 4482/5692 [00:13<00:03, 337.88it/s]\u001b[A\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 4519/5692 [00:13<00:03, 346.98it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 4554/5692 [00:13<00:03, 332.81it/s]\u001b[A\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 4591/5692 [00:13<00:03, 336.38it/s]\u001b[A\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 4626/5692 [00:13<00:03, 339.27it/s]\u001b[A\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 4661/5692 [00:13<00:03, 338.31it/s]\u001b[A\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 4695/5692 [00:13<00:03, 324.79it/s]\u001b[A\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 4729/5692 [00:13<00:02, 328.12it/s]\u001b[A\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 4765/5692 [00:14<00:02, 335.68it/s]\u001b[A\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 4799/5692 [00:14<00:02, 333.65it/s]\u001b[A\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 4833/5692 [00:14<00:02, 324.44it/s]\u001b[A\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 4866/5692 [00:14<00:02, 317.81it/s]\u001b[A\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 4898/5692 [00:14<00:02, 317.83it/s]\u001b[A\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4933/5692 [00:14<00:02, 326.65it/s]\u001b[A\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 4969/5692 [00:14<00:02, 326.73it/s]\u001b[A\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 5008/5692 [00:14<00:01, 344.77it/s]\u001b[A\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 5043/5692 [00:14<00:01, 344.87it/s]\u001b[A\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 5081/5692 [00:14<00:01, 353.04it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 5117/5692 [00:15<00:01, 350.36it/s]\u001b[A\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 5156/5692 [00:15<00:01, 360.42it/s]\u001b[A\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 5193/5692 [00:15<00:01, 347.19it/s]\u001b[A\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 5228/5692 [00:15<00:01, 336.13it/s]\u001b[A\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5266/5692 [00:15<00:01, 344.93it/s]\u001b[A\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 5301/5692 [00:15<00:01, 330.70it/s]\u001b[A\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 5335/5692 [00:15<00:01, 324.01it/s]\u001b[A\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 5371/5692 [00:15<00:00, 331.92it/s]\u001b[A\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 5405/5692 [00:15<00:00, 326.56it/s]\u001b[A\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5442/5692 [00:16<00:00, 337.62it/s]\u001b[A\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 5480/5692 [00:16<00:00, 349.16it/s]\u001b[A\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 5516/5692 [00:16<00:00, 351.21it/s]\u001b[A\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 5552/5692 [00:16<00:00, 340.44it/s]\u001b[A\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 5587/5692 [00:16<00:00, 340.21it/s]\u001b[A\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 5622/5692 [00:16<00:00, 332.08it/s]\u001b[A\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 5656/5692 [00:16<00:00, 320.06it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5692/5692 [00:16<00:00, 338.13it/s]\u001b[A\nINFO:utils_qa:Saving predictions to /kaggle/working/phobert-base/eval_predictions.json.\nINFO:utils_qa:Saving nbest_preds to /kaggle/working/phobert-base/eval_nbest_predictions.json.\nINFO:utils_qa:Saving null_odds to /kaggle/working/phobert-base/eval_null_odds.json.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [01:26<00:00,  7.77it/s]\n***** eval metrics *****\n  epoch                   =        2.0\n  eval_HasAns_exact       =    38.8889\n  eval_HasAns_f1          =    58.9549\n  eval_HasAns_total       =       3852\n  eval_NoAns_exact        =    35.2717\n  eval_NoAns_f1           =    35.2717\n  eval_NoAns_total        =       1840\n  eval_best_exact         =    37.7547\n  eval_best_exact_thresh  =        0.0\n  eval_best_f1            =    51.3342\n  eval_best_f1_thresh     =        0.0\n  eval_exact              =    37.7196\n  eval_f1                 =    51.2991\n  eval_runtime            = 0:01:01.51\n  eval_samples            =       8027\n  eval_samples_per_second =    130.489\n  eval_steps_per_second   =     10.875\n  eval_total              =       5692\n[INFO|modelcard.py:456] 2026-01-05 16:21:01,266 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mphobert-base-2epoch-bs32\u001b[0m at: \u001b[34m\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20260105_155601-83f86it1/logs\u001b[0m\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"OUTPUT_DIR=\"/kaggle/working/phobert-base\"\n\n!python run_qa.py \\\n  --model_name_or_path $OUTPUT_DIR \\\n  --test_file \"/kaggle/input/uit-viquad-2-0/Private_Test_ref.json\" \\\n  --do_predict \\\n  --per_device_eval_batch_size 16 \\\n  --max_seq_length 256 \\\n  --doc_stride 128 \\\n  --dataloader_num_workers 4 \\\n  --output_dir $OUTPUT_DIR \\\n  --version_2_with_negative \\\n  --n_best_size 20 \\\n  --max_answer_length 30 \\\n  --null_score_diff_threshold 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:21:39.618380Z","iopub.execute_input":"2026-01-05T16:21:39.619093Z","iopub.status.idle":"2026-01-05T16:23:40.817705Z","shell.execute_reply.started":"2026-01-05T16:21:39.619053Z","shell.execute_reply":"2026-01-05T16:23:40.816957Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2026-01-05 16:21:44.223270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767630104.245137     525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767630104.252846     525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767630104.270603     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767630104.270632     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767630104.270637     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767630104.270643     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nWARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\nINFO:__main__:Training/evaluation parameters TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=True,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=4,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=False,\ndo_predict=True,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=None,\neval_strategy=IntervalStrategy.NO,\neval_use_gather_object=False,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=None,\nhub_revision=None,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=no,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nliger_kernel_config=None,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=/kaggle/working/phobert-base/runs/Jan05_16-21-49_69d344501b91,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_kwargs={},\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=OptimizerNames.ADAMW_TORCH_FUSED,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=/kaggle/working/phobert-base,\noverwrite_output_dir=False,\nparallelism_config=None,\npast_index=-1,\nper_device_eval_batch_size=16,\nper_device_train_batch_size=8,\nprediction_loss_only=False,\nproject=huggingface,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=None,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=SaveStrategy.STEPS,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\ntrackio_space_id=trackio,\nuse_cpu=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.0,\n)\nUsing custom data configuration default-aecb69af4c19754c\nINFO:datasets.builder:Using custom data configuration default-aecb69af4c19754c\nGenerating dataset json (/root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nINFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nINFO:datasets.builder:Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nDownloading took 0.0 min\nINFO:datasets.download.download_manager:Downloading took 0.0 min\nChecksum Computation took 0.0 min\nINFO:datasets.download.download_manager:Checksum Computation took 0.0 min\nGenerating test split\nINFO:datasets.builder:Generating test split\nGenerating test split: 7301 examples [00:00, 14974.95 examples/s]\nUnable to verify splits sizes.\nINFO:datasets.utils.info_utils:Unable to verify splits sizes.\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\nINFO:datasets.builder:Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\n[INFO|configuration_utils.py:763] 2026-01-05 16:21:51,901 >> loading configuration file /kaggle/working/phobert-base/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 16:21:51,902 >> Model config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"dtype\": \"float32\",\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 258,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"tokenizer_class\": \"PhobertTokenizer\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 64001\n}\n\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 16:21:51,937 >> loading file vocab.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 16:21:51,937 >> loading file merges.txt\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 16:21:51,937 >> loading file tokenizer.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 16:21:51,938 >> loading file added_tokens.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 16:21:51,938 >> loading file special_tokens_map.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 16:21:51,938 >> loading file tokenizer_config.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 16:21:51,938 >> loading file chat_template.jinja\n[INFO|modeling_utils.py:1169] 2026-01-05 16:21:52,100 >> loading weights file /kaggle/working/phobert-base/model.safetensors\nRunning tokenizer on prediction dataset:   0%|  | 0/7301 [00:00<?, ? examples/s]WARNING:__main__:Clipped token IDs in 28 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nCaching processed dataset at /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-49a80fd860cbfaa6.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-49a80fd860cbfaa6.arrow\nRunning tokenizer on prediction dataset:  14%|‚ñè| 1000/7301 [00:01<00:07, 808.28 WARNING:__main__:Clipped token IDs in 28 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on prediction dataset:  27%|‚ñé| 2000/7301 [00:01<00:05, 1052.54WARNING:__main__:Clipped token IDs in 8 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on prediction dataset:  55%|‚ñå| 4000/7301 [00:03<00:03, 1084.57WARNING:__main__:Clipped token IDs in 14 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on prediction dataset:  96%|‚ñâ| 7000/7301 [00:06<00:00, 1227.83WARNING:__main__:Clipped token IDs in 37 examples to fit model vocab_size (64001). This may occur when using RobertaTokenizerFast fallback for PhoBERT.\nRunning tokenizer on prediction dataset: 100%|‚ñà| 7301/7301 [00:06<00:00, 1120.57\nINFO:__main__:*** Predict ***\n[INFO|trainer.py:1012] 2026-01-05 16:21:59,947 >> The following columns in the test set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4643] 2026-01-05 16:21:59,957 >> \n***** Running Prediction *****\n[INFO|trainer.py:4645] 2026-01-05 16:21:59,957 >>   Num examples = 10117\n[INFO|trainer.py:4648] 2026-01-05 16:21:59,957 >>   Batch size = 16\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 632/633 [01:08<00:00,  9.26it/s]INFO:utils_qa:Post-processing 7301 example predictions split into 10117 features.\n\n  0%|                                                  | 0/7301 [00:00<?, ?it/s]\u001b[A\n  1%|‚ñè                                       | 41/7301 [00:00<00:18, 402.87it/s]\u001b[A\n  1%|‚ñå                                       | 93/7301 [00:00<00:15, 468.69it/s]\u001b[A\n  2%|‚ñã                                      | 140/7301 [00:00<00:15, 463.89it/s]\u001b[A\n  3%|‚ñâ                                      | 187/7301 [00:00<00:22, 319.51it/s]\u001b[A\n  3%|‚ñà‚ñè                                     | 224/7301 [00:00<00:32, 218.14it/s]\u001b[A\n  3%|‚ñà‚ñé                                     | 252/7301 [00:01<00:41, 168.55it/s]\u001b[A\n  4%|‚ñà‚ñå                                     | 283/7301 [00:01<00:36, 192.77it/s]\u001b[A\n  4%|‚ñà‚ñã                                     | 323/7301 [00:01<00:29, 234.17it/s]\u001b[A\n  5%|‚ñà‚ñâ                                     | 369/7301 [00:01<00:24, 284.34it/s]\u001b[A\n  6%|‚ñà‚ñà‚ñè                                    | 414/7301 [00:01<00:21, 321.49it/s]\u001b[A\n  6%|‚ñà‚ñà‚ñç                                    | 462/7301 [00:01<00:18, 361.34it/s]\u001b[A\n  7%|‚ñà‚ñà‚ñä                                    | 515/7301 [00:01<00:16, 405.08it/s]\u001b[A\n  8%|‚ñà‚ñà‚ñâ                                    | 560/7301 [00:01<00:16, 414.18it/s]\u001b[A\n  8%|‚ñà‚ñà‚ñà‚ñè                                   | 605/7301 [00:01<00:16, 400.82it/s]\u001b[A\n  9%|‚ñà‚ñà‚ñà‚ñç                                   | 648/7301 [00:02<00:16, 408.62it/s]\u001b[A\n 10%|‚ñà‚ñà‚ñà‚ñã                                   | 699/7301 [00:02<00:15, 435.63it/s]\u001b[A\n 10%|‚ñà‚ñà‚ñà‚ñâ                                   | 747/7301 [00:02<00:14, 446.22it/s]\u001b[A\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 793/7301 [00:02<00:15, 424.15it/s]\u001b[A\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 837/7301 [00:02<00:17, 363.12it/s]\u001b[A\n 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 887/7301 [00:02<00:16, 395.99it/s]\u001b[A\n 13%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 929/7301 [00:02<00:17, 370.96it/s]\u001b[A\n 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 972/7301 [00:02<00:16, 383.63it/s]\u001b[A\n 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 1012/7301 [00:02<00:16, 384.70it/s]\u001b[A\n 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 1052/7301 [00:03<00:17, 357.84it/s]\u001b[A\n 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 1089/7301 [00:03<00:17, 355.89it/s]\u001b[A\n 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 1126/7301 [00:03<00:17, 345.02it/s]\u001b[A\n 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 1161/7301 [00:03<00:17, 345.80it/s]\u001b[A\n 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 1201/7301 [00:03<00:16, 359.14it/s]\u001b[A\n 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 1240/7301 [00:03<00:16, 367.83it/s]\u001b[A\n 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 1292/7301 [00:03<00:14, 409.94it/s]\u001b[A\n 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 1340/7301 [00:03<00:13, 427.84it/s]\u001b[A\n 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 1388/7301 [00:03<00:13, 443.08it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 1434/7301 [00:04<00:13, 447.45it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 1479/7301 [00:04<00:15, 369.85it/s]\u001b[A\n 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 1522/7301 [00:04<00:15, 383.86it/s]\u001b[A\n 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 1563/7301 [00:04<00:15, 364.45it/s]\u001b[A\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 1607/7301 [00:04<00:14, 383.45it/s]\u001b[A\n 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 1652/7301 [00:04<00:14, 400.50it/s]\u001b[A\n 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1695/7301 [00:04<00:13, 406.31it/s]\u001b[A\n 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1737/7301 [00:04<00:14, 391.76it/s]\u001b[A\n 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1777/7301 [00:04<00:14, 377.56it/s]\u001b[A\n 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1816/7301 [00:05<00:14, 370.18it/s]\u001b[A\n 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1854/7301 [00:05<00:14, 367.48it/s]\u001b[A\n 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1901/7301 [00:05<00:13, 392.47it/s]\u001b[A\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1948/7301 [00:05<00:12, 412.63it/s]\u001b[A\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1990/7301 [00:05<00:13, 387.38it/s]\u001b[A\n 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 2030/7301 [00:05<00:13, 377.03it/s]\u001b[A\n 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 2074/7301 [00:05<00:13, 393.08it/s]\u001b[A\n 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 2114/7301 [00:05<00:13, 388.87it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 2158/7301 [00:05<00:12, 399.31it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 2202/7301 [00:06<00:12, 408.36it/s]\u001b[A\n 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 2243/7301 [00:06<00:12, 402.25it/s]\u001b[A\n 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 2286/7301 [00:06<00:12, 406.24it/s]\u001b[A\n 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 2327/7301 [00:06<00:12, 389.59it/s]\u001b[A\n 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 2367/7301 [00:06<00:13, 363.83it/s]\u001b[A\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 2410/7301 [00:06<00:12, 380.11it/s]\u001b[A\n 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 2453/7301 [00:06<00:12, 393.04it/s]\u001b[A\n 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 2493/7301 [00:06<00:12, 381.06it/s]\u001b[A\n 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 2532/7301 [00:06<00:12, 374.02it/s]\u001b[A\n 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 2570/7301 [00:07<00:13, 340.88it/s]\u001b[A\n 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 2605/7301 [00:07<00:14, 320.72it/s]\u001b[A\n 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 2638/7301 [00:07<00:14, 319.88it/s]\u001b[A\n 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 2676/7301 [00:07<00:13, 333.10it/s]\u001b[A\n 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 2712/7301 [00:07<00:13, 339.70it/s]\u001b[A\n 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 2747/7301 [00:07<00:13, 336.75it/s]\u001b[A\n 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 2781/7301 [00:07<00:13, 324.23it/s]\u001b[A\n 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 2814/7301 [00:07<00:14, 305.59it/s]\u001b[A\n 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 2845/7301 [00:07<00:15, 290.80it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 2893/7301 [00:08<00:12, 341.68it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 2929/7301 [00:08<00:12, 345.90it/s]\u001b[A\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 2973/7301 [00:08<00:11, 371.48it/s]\u001b[A\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 3016/7301 [00:08<00:11, 386.21it/s]\u001b[A\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 3056/7301 [00:08<00:11, 374.75it/s]\u001b[A\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 3106/7301 [00:08<00:10, 410.09it/s]\u001b[A\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 3148/7301 [00:08<00:10, 396.77it/s]\u001b[A\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 3189/7301 [00:08<00:11, 347.17it/s]\u001b[A\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 3229/7301 [00:08<00:11, 360.58it/s]\u001b[A\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 3267/7301 [00:09<00:11, 361.58it/s]\u001b[A\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 3308/7301 [00:09<00:10, 374.56it/s]\u001b[A\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 3347/7301 [00:09<00:12, 328.79it/s]\u001b[A\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 3382/7301 [00:09<00:12, 322.16it/s]\u001b[A\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 3418/7301 [00:09<00:11, 327.66it/s]\u001b[A\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 3452/7301 [00:09<00:12, 297.33it/s]\u001b[A\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 3487/7301 [00:09<00:12, 310.46it/s]\u001b[A\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 3519/7301 [00:09<00:12, 310.76it/s]\u001b[A\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 3551/7301 [00:09<00:12, 296.57it/s]\u001b[A\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 3586/7301 [00:10<00:12, 308.96it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 3618/7301 [00:10<00:12, 303.84it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 3655/7301 [00:10<00:11, 320.72it/s]\u001b[A\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 3692/7301 [00:10<00:10, 333.34it/s]\u001b[A\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 3728/7301 [00:10<00:10, 338.65it/s]\u001b[A\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 3763/7301 [00:10<00:11, 321.35it/s]\u001b[A\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 3800/7301 [00:10<00:10, 334.11it/s]\u001b[A\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 3834/7301 [00:10<00:11, 298.70it/s]\u001b[A\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 3865/7301 [00:10<00:12, 273.50it/s]\u001b[A\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 3894/7301 [00:11<00:13, 259.88it/s]\u001b[A\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 3936/7301 [00:11<00:11, 300.64it/s]\u001b[A\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 3968/7301 [00:11<00:12, 274.85it/s]\u001b[A\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 4000/7301 [00:11<00:11, 286.13it/s]\u001b[A\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 4030/7301 [00:11<00:11, 276.00it/s]\u001b[A\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 4071/7301 [00:11<00:10, 310.06it/s]\u001b[A\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 4117/7301 [00:11<00:09, 349.36it/s]\u001b[A\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 4153/7301 [00:11<00:09, 345.93it/s]\u001b[A\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 4189/7301 [00:12<00:09, 312.54it/s]\u001b[A\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 4225/7301 [00:12<00:09, 324.84it/s]\u001b[A\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 4259/7301 [00:12<00:09, 325.28it/s]\u001b[A\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 4300/7301 [00:12<00:08, 348.75it/s]\u001b[A\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 4340/7301 [00:12<00:08, 360.60it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 4377/7301 [00:12<00:09, 299.17it/s]\u001b[A\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 4419/7301 [00:12<00:08, 327.95it/s]\u001b[A\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 4460/7301 [00:12<00:08, 346.89it/s]\u001b[A\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 4497/7301 [00:12<00:08, 345.17it/s]\u001b[A\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 4533/7301 [00:13<00:08, 344.90it/s]\u001b[A\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 4569/7301 [00:13<00:08, 339.85it/s]\u001b[A\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 4606/7301 [00:13<00:07, 347.73it/s]\u001b[A\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 4644/7301 [00:13<00:07, 355.61it/s]\u001b[A\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 4692/7301 [00:13<00:06, 391.54it/s]\u001b[A\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 4740/7301 [00:13<00:06, 416.17it/s]\u001b[A\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 4792/7301 [00:13<00:05, 444.47it/s]\u001b[A\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 4840/7301 [00:13<00:05, 454.80it/s]\u001b[A\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 4886/7301 [00:13<00:05, 429.02it/s]\u001b[A\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 4930/7301 [00:13<00:05, 424.18it/s]\u001b[A\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 4978/7301 [00:14<00:05, 437.65it/s]\u001b[A\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 5023/7301 [00:14<00:05, 387.26it/s]\u001b[A\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 5065/7301 [00:14<00:05, 395.59it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 5112/7301 [00:14<00:05, 414.23it/s]\u001b[A\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 5160/7301 [00:14<00:04, 430.31it/s]\u001b[A\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 5204/7301 [00:14<00:05, 418.70it/s]\u001b[A\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 5247/7301 [00:14<00:05, 382.60it/s]\u001b[A\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 5296/7301 [00:14<00:04, 409.88it/s]\u001b[A\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 5342/7301 [00:14<00:04, 422.78it/s]\u001b[A\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 5390/7301 [00:15<00:04, 437.24it/s]\u001b[A\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 5438/7301 [00:15<00:04, 449.39it/s]\u001b[A\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 5484/7301 [00:15<00:04, 394.57it/s]\u001b[A\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 5525/7301 [00:15<00:04, 382.50it/s]\u001b[A\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 5571/7301 [00:15<00:04, 401.94it/s]\u001b[A\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 5615/7301 [00:15<00:04, 409.06it/s]\u001b[A\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 5657/7301 [00:15<00:04, 393.08it/s]\u001b[A\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 5697/7301 [00:15<00:04, 331.29it/s]\u001b[A\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 5737/7301 [00:16<00:04, 346.89it/s]\u001b[A\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 5774/7301 [00:16<00:04, 352.53it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 5820/7301 [00:16<00:03, 381.13it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 5860/7301 [00:16<00:03, 377.99it/s]\u001b[A\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 5910/7301 [00:16<00:03, 409.98it/s]\u001b[A\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 5952/7301 [00:16<00:03, 383.80it/s]\u001b[A\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 5992/7301 [00:16<00:03, 361.41it/s]\u001b[A\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 6040/7301 [00:16<00:03, 391.52it/s]\u001b[A\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 6080/7301 [00:16<00:03, 393.33it/s]\u001b[A\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 6120/7301 [00:17<00:03, 383.36it/s]\u001b[A\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 6161/7301 [00:17<00:02, 389.14it/s]\u001b[A\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 6201/7301 [00:17<00:02, 390.87it/s]\u001b[A\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 6241/7301 [00:17<00:02, 357.63it/s]\u001b[A\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 6284/7301 [00:17<00:02, 376.96it/s]\u001b[A\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 6323/7301 [00:17<00:02, 374.96it/s]\u001b[A\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 6361/7301 [00:17<00:02, 376.23it/s]\u001b[A\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 6399/7301 [00:17<00:02, 356.01it/s]\u001b[A\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 6447/7301 [00:17<00:02, 389.01it/s]\u001b[A\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 6488/7301 [00:17<00:02, 394.97it/s]\u001b[A\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 6528/7301 [00:18<00:02, 366.81it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 6566/7301 [00:18<00:02, 363.23it/s]\u001b[A\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6619/7301 [00:18<00:01, 409.67it/s]\u001b[A\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6663/7301 [00:18<00:01, 417.91it/s]\u001b[A\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 6706/7301 [00:18<00:01, 412.64it/s]\u001b[A\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 6749/7301 [00:18<00:01, 416.09it/s]\u001b[A\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 6792/7301 [00:18<00:01, 418.19it/s]\u001b[A\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6836/7301 [00:18<00:01, 422.14it/s]\u001b[A\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 6879/7301 [00:18<00:01, 397.86it/s]\u001b[A\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 6932/7301 [00:19<00:00, 434.90it/s]\u001b[A\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 6976/7301 [00:19<00:00, 426.05it/s]\u001b[A\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 7019/7301 [00:19<00:00, 421.66it/s]\u001b[A\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7062/7301 [00:19<00:00, 406.58it/s]\u001b[A\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 7103/7301 [00:19<00:00, 364.12it/s]\u001b[A\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 7141/7301 [00:19<00:00, 355.33it/s]\u001b[A\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 7178/7301 [00:19<00:00, 335.28it/s]\u001b[A\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 7213/7301 [00:19<00:00, 312.37it/s]\u001b[A\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 7245/7301 [00:20<00:00, 309.06it/s]\u001b[A\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7301/7301 [00:20<00:00, 361.46it/s]\u001b[A\nINFO:utils_qa:Saving predictions to /kaggle/working/phobert-base/predict_predictions.json.\nINFO:utils_qa:Saving nbest_preds to /kaggle/working/phobert-base/predict_nbest_predictions.json.\nINFO:utils_qa:Saving null_odds to /kaggle/working/phobert-base/predict_null_odds.json.\n***** predict metrics *****\n  predict_samples             =      10117\n  test_NoAns_exact            =    31.6395\n  test_NoAns_f1               =    31.6395\n  test_NoAns_total            =       7301\n  test_best_exact             =      100.0\n  test_best_exact_thresh      =        0.0\n  test_best_f1                =      100.0\n  test_best_f1_thresh         =        0.0\n  test_exact                  =    31.6395\n  test_f1                     =    31.6395\n  test_model_preparation_time =     0.0026\n  test_runtime                = 0:01:08.86\n  test_samples_per_second     =    146.905\n  test_steps_per_second       =      9.192\n  test_total                  =       7301\n[INFO|modelcard.py:456] 2026-01-05 16:23:38,491 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 633/633 [01:38<00:00,  6.46it/s]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"OUTPUT_DIR=\"/kaggle/working/phobert-base\"\n\n!python evaluation.py \\\n  \"/kaggle/input/uit-viquad-2-0/ground_truth_private_test.json\" \\\n  \"$OUTPUT_DIR/predict_predictions.json\" \\\n  --na-prob-file \"$OUTPUT_DIR/predict_null_odds.json\" \\\n  --out-file \"$OUTPUT_DIR/evaluation_results.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:23:46.382232Z","iopub.execute_input":"2026-01-05T16:23:46.382919Z","iopub.status.idle":"2026-01-05T16:23:47.285087Z","shell.execute_reply.started":"2026-01-05T16:23:46.382884Z","shell.execute_reply":"2026-01-05T16:23:47.284431Z"}},"outputs":[{"name":"stdout","text":"2026-01-05 16:23:46 - INFO - Logger initialized. Log file: ./logs/evaluation_20260105_162346.log\n2026-01-05 16:23:46 - INFO - Loading dataset from /kaggle/input/uit-viquad-2-0/ground_truth_private_test.json\n2026-01-05 16:23:46 - INFO - Loading predictions from /kaggle/working/phobert-base/predict_predictions.json\n2026-01-05 16:23:46 - INFO - Loading no-answer probabilities from /kaggle/working/phobert-base/predict_null_odds.json\n2026-01-05 16:23:46 - INFO - Dataset contains 19 examples\n2026-01-05 16:23:46 - INFO - Predictions contain 7301 entries\n2026-01-05 16:23:47 - INFO - Evaluating on 3712 questions with predictions\n2026-01-05 16:23:47 - INFO -   Has answer: 2596\n2026-01-05 16:23:47 - INFO -   No answer: 1116\n2026-01-05 16:23:47 - INFO - Evaluation Results:\n2026-01-05 16:23:47 - INFO -   Exact Match: 43.67%\n2026-01-05 16:23:47 - INFO -   F1 Score: 52.65%\n2026-01-05 16:23:47 - INFO -   HasAns - Exact: 41.26%\n2026-01-05 16:23:47 - INFO -   HasAns - F1: 54.09%\n2026-01-05 16:23:47 - INFO -   NoAns - Exact: 49.28%\n2026-01-05 16:23:47 - INFO -   NoAns - F1: 49.28%\n2026-01-05 16:23:47 - INFO - Saving evaluation results to /kaggle/working/phobert-base/evaluation_results.json\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# !rm -rf /kaggle/working/DS310\n# %cd /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:55:16.149911Z","iopub.execute_input":"2026-01-05T15:55:16.150553Z","iopub.status.idle":"2026-01-05T15:55:16.278524Z","shell.execute_reply.started":"2026-01-05T15:55:16.150524Z","shell.execute_reply":"2026-01-05T15:55:16.277635Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}