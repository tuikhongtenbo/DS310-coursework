{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38930e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN\n",
    "# ==========================================\n",
    "%pip install -q transformers datasets accelerate evaluate sentencepiece vncorenlp\n",
    "\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. C·∫§U H√åNH VNCORENLP\n",
    "# ==========================================\n",
    "# ƒê∆∞·ªùng d·∫´n file jar b·∫°n ƒë√£ upload\n",
    "VNCORENLP_JAR = '/content/vncorenlp/VnCoreNLP-1.1.1.jar'\n",
    "\n",
    "try:\n",
    "    # Kh·ªüi t·∫°o VnCoreNLP (ch·ªâ d√πng annotator 'wseg' ƒë·ªÉ t√°ch t·ª´)\n",
    "    rdrsegmenter = VnCoreNLP(VNCORENLP_JAR, annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "    print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o th√†nh c√¥ng VnCoreNLP!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå L·ªói kh·ªüi t·∫°o VnCoreNLP. Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file .jar\")\n",
    "    print(e)\n",
    "    # T·∫°o dummy function n·∫øu kh√¥ng load ƒë∆∞·ª£c ƒë·ªÉ code kh√¥ng crash\n",
    "    class DummySegmenter:\n",
    "        def tokenize(self, text): return [[text]]\n",
    "    rdrsegmenter = DummySegmenter()\n",
    "\n",
    "def segment_text(text):\n",
    "    \"\"\"H√†m t√°ch t·ª´: Input 'H√† N·ªôi' -> Output 'H√†_N·ªôi'\"\"\"\n",
    "    if not text: return \"\"\n",
    "    try:\n",
    "        sentences = rdrsegmenter.tokenize(text)\n",
    "        # N·ªëi c√°c t·ª´ l·∫°i b·∫±ng kho·∫£ng tr·∫Øng (VnCoreNLP d√πng _ n·ªëi t·ª´ gh√©p)\n",
    "        return \" \".join([\" \".join(s) for s in sentences])\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. H√ÄM LOAD & S·ª¨A L·ªñI JSON (SI√äU M·∫†NH - C·∫¢I TI·∫æN)\n",
    "# ==========================================\n",
    "def load_json_robust(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON v·ªõi x·ª≠ l√Ω c√°c l·ªói escape kh√¥ng h·ª£p l·ªá\n",
    "    ƒê·∫∑c bi·ªát x·ª≠ l√Ω c√°c k√Ω t·ª± LaTeX nh∆∞ \\displaystyle, \\in, etc.\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ ƒêang ƒë·ªçc file: {file_path} ...\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # --- B∆Ø·ªöC S·ª¨A L·ªñI REGEX (C·∫¢I TI·∫æN) ---\n",
    "    \n",
    "    # 1. S·ª≠a l·ªói \\u kh√¥ng h·ª£p l·ªá (v√≠ d·ª•: ƒë∆∞·ªùng d·∫´n C:\\user -> C:\\\\user)\n",
    "    # Ch·ªâ s·ª≠a \\u kh√¥ng theo sau b·ªüi 4 hex digits\n",
    "    content = re.sub(r'\\\\u(?![0-9a-fA-F]{4})', r'\\\\\\\\u', content)\n",
    "    \n",
    "    # 2. S·ª≠a l·ªói escape kh√¥ng h·ª£p l·ªá trong chu·ªói JSON\n",
    "    # JSON ch·ªâ cho ph√©p c√°c escape: \", \\, /, b, f, n, r, t, u\n",
    "    # T·∫•t c·∫£ c√°c \\ kh√°c c·∫ßn ƒë∆∞·ª£c escape th√†nh \\\\\n",
    "    \n",
    "    # Strategy: T√¨m t·∫•t c·∫£ c√°c \\ trong chu·ªói (kh√¥ng ph·∫£i trong escape sequence h·ª£p l·ªá)\n",
    "    # v√† escape ch√∫ng\n",
    "    \n",
    "    # Tr∆∞·ªõc ti√™n, b·∫£o v·ªá c√°c escape sequence h·ª£p l·ªá b·∫±ng placeholder\n",
    "    valid_escapes = {\n",
    "        r'\\\\\"': '__QUOTE_ESCAPE__',\n",
    "        r'\\\\\\\\': '__BACKSLASH_ESCAPE__',\n",
    "        r'\\\\/': '__SLASH_ESCAPE__',\n",
    "        r'\\\\b': '__BS_ESCAPE__',\n",
    "        r'\\\\f': '__FF_ESCAPE__',\n",
    "        r'\\\\n': '__LF_ESCAPE__',\n",
    "        r'\\\\r': '__CR_ESCAPE__',\n",
    "        r'\\\\t': '__TAB_ESCAPE__',\n",
    "        r'\\\\u[0-9a-fA-F]{4}': '__UNICODE_ESCAPE__',  # \\uXXXX\n",
    "    }\n",
    "    \n",
    "    # B·∫£o v·ªá c√°c escape h·ª£p l·ªá\n",
    "    protected_content = content\n",
    "    for pattern, placeholder in valid_escapes.items():\n",
    "        if pattern == r'\\\\u[0-9a-fA-F]{4}':\n",
    "            # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho unicode escape\n",
    "            protected_content = re.sub(r'\\\\u([0-9a-fA-F]{4})', r'__UNICODE_ESCAPE__\\1', protected_content)\n",
    "        else:\n",
    "            protected_content = protected_content.replace(pattern, placeholder)\n",
    "    \n",
    "    # B√¢y gi·ªù escape t·∫•t c·∫£ c√°c \\ c√≤n l·∫°i (kh√¥ng ph·∫£i l√† escape h·ª£p l·ªá)\n",
    "    # T√¨m c√°c \\ ƒë·ª©ng m·ªôt m√¨nh ho·∫∑c theo sau b·ªüi k√Ω t·ª± kh√¥ng h·ª£p l·ªá\n",
    "    protected_content = re.sub(r'\\\\(?![\\\\\"/bfnrtu])', r'\\\\\\\\', protected_content)\n",
    "    \n",
    "    # Kh√¥i ph·ª•c c√°c escape h·ª£p l·ªá\n",
    "    for pattern, placeholder in valid_escapes.items():\n",
    "        if pattern == r'\\\\u[0-9a-fA-F]{4}':\n",
    "            protected_content = re.sub(r'__UNICODE_ESCAPE__([0-9a-fA-F]{4})', r'\\\\u\\1', protected_content)\n",
    "        else:\n",
    "            protected_content = protected_content.replace(placeholder, pattern)\n",
    "    \n",
    "    # 3. X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: c√°c k√Ω t·ª± LaTeX th∆∞·ªùng g·∫∑p\n",
    "    # V√≠ d·ª•: \\displaystyle, \\in, \\sum, \\int, etc.\n",
    "    # Nh·ªØng c√°i n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü b∆∞·ªõc 2, nh∆∞ng c√≥ th·ªÉ c·∫ßn cleanup th√™m\n",
    "    \n",
    "    # 4. Cleanup: X·ª≠ l√Ω c√°c d·∫•u \\\\\\\\ ch·ªìng ch√©o (n·∫øu c√≥)\n",
    "    # Gi·∫£m \\\\\\\\ th√†nh \\\\ (nh∆∞ng gi·ªØ \\\\\\\\ n·∫øu l√† escape h·ª£p l·ªá c·ªßa \\)\n",
    "    protected_content = re.sub(r'\\\\\\\\{3,}', r'\\\\\\\\', protected_content)\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(protected_content)\n",
    "        print(\"‚úÖ Parse JSON th√†nh c√¥ng!\")\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå V·∫´n l·ªói JSON t·∫°i v·ªã tr√≠ {e.pos}: {e.msg}\")\n",
    "        start = max(0, e.pos - 200)\n",
    "        end = min(len(protected_content), e.pos + 200)\n",
    "        print(f\"Context l·ªói: ...{protected_content[start:end]}...\")\n",
    "        \n",
    "        # Th·ª≠ c√°ch kh√°c: x·ª≠ l√Ω t·ª´ng d√≤ng\n",
    "        print(\"\\nüîÑ Th·ª≠ ph∆∞∆°ng ph√°p x·ª≠ l√Ω t·ª´ng d√≤ng...\")\n",
    "        try:\n",
    "            lines = protected_content.split('\\n')\n",
    "            fixed_lines = []\n",
    "            for i, line in enumerate(lines):\n",
    "                # Escape c√°c \\ kh√¥ng h·ª£p l·ªá trong m·ªói d√≤ng\n",
    "                fixed_line = re.sub(r'\\\\(?![\\\\\"/bfnrtu])', r'\\\\\\\\', line)\n",
    "                fixed_lines.append(fixed_line)\n",
    "            fixed_content = '\\n'.join(fixed_lines)\n",
    "            data = json.loads(fixed_content)\n",
    "            print(\"‚úÖ Parse JSON th√†nh c√¥ng b·∫±ng ph∆∞∆°ng ph√°p x·ª≠ l√Ω t·ª´ng d√≤ng!\")\n",
    "            return data\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå V·∫´n l·ªói: {e2}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU: SEGMENTATION & REMAP\n",
    "# ==========================================\n",
    "\n",
    "def preprocess_dataset_with_segmentation(raw_data_list):\n",
    "    \"\"\"\n",
    "    Ch·∫°y t√°ch t·ª´ cho to√†n b·ªô dataset v√† t√≠nh to√°n l·∫°i answer_start\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    print(\"‚è≥ ƒêang ch·∫°y VnCoreNLP t√°ch t·ª´ v√† map l·∫°i v·ªã tr√≠ (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...\")\n",
    "    \n",
    "    for idx, item in enumerate(raw_data_list):\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"  ƒêang x·ª≠ l√Ω: {idx}/{len(raw_data_list)}...\")\n",
    "        \n",
    "        context_raw = item['context']\n",
    "        question_raw = item['question']\n",
    "        \n",
    "        # 1. T√°ch t·ª´ Context v√† Question\n",
    "        context_seg = segment_text(context_raw)\n",
    "        question_seg = segment_text(question_raw)\n",
    "        \n",
    "        # 2. X·ª≠ l√Ω Answer\n",
    "        # V√¨ t√°ch t·ª´ l√†m thay ƒë·ªïi ƒë·ªô d√†i chu·ªói, ta ph·∫£i t√¨m l·∫°i v·ªã tr√≠ c√¢u tr·∫£ l·ªùi trong chu·ªói m·ªõi\n",
    "        answers_new = {\"text\": [], \"answer_start\": []}\n",
    "        \n",
    "        if \"answers\" in item and item[\"answers\"][\"text\"]:\n",
    "            text_raw = item[\"answers\"][\"text\"][0]\n",
    "            \n",
    "            # T√°ch t·ª´ c√¢u tr·∫£ l·ªùi\n",
    "            text_seg = segment_text(text_raw)\n",
    "            \n",
    "            # T√¨m v·ªã tr√≠ ch√≠nh x√°c c·ªßa c√¢u tr·∫£ l·ªùi ƒë√£ t√°ch t·ª´ trong context ƒë√£ t√°ch t·ª´\n",
    "            new_start = context_seg.find(text_seg)\n",
    "            \n",
    "            if new_start != -1:\n",
    "                answers_new[\"text\"] = [text_seg]\n",
    "                answers_new[\"answer_start\"] = [new_start]\n",
    "                \n",
    "                # L∆∞u data m·ªõi\n",
    "                new_item = {\n",
    "                    \"id\": item[\"id\"],\n",
    "                    \"context\": context_seg,   # D√πng context ƒë√£ t√°ch t·ª´\n",
    "                    \"question\": question_seg, # D√πng question ƒë√£ t√°ch t·ª´\n",
    "                    \"answers\": answers_new\n",
    "                }\n",
    "                processed_data.append(new_item)\n",
    "            else:\n",
    "                # N·∫øu kh√¥ng t√¨m th·∫•y (do t√°ch t·ª´ l√†m bi·∫øn d·∫°ng qu√° nhi·ªÅu), b·ªè qua m·∫´u n√†y ƒë·ªÉ tr√°nh l·ªói training\n",
    "                skipped_count += 1\n",
    "                # Fallback: c·ªë g·∫Øng d√πng l·∫°i raw n·∫øu c·∫ßn, nh∆∞ng t·ªët nh·∫•t l√† skip ƒë·ªÉ s·∫°ch data\n",
    "        else:\n",
    "            # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi\n",
    "            new_item = {\n",
    "                \"id\": item[\"id\"],\n",
    "                \"context\": context_seg,\n",
    "                \"question\": question_seg,\n",
    "                \"answers\": {\"text\": [], \"answer_start\": []}\n",
    "            }\n",
    "            processed_data.append(new_item)\n",
    "\n",
    "    print(f\"‚úÖ X·ª≠ l√Ω xong. T·ªïng: {len(raw_data_list)}. H·ª£p l·ªá: {len(processed_data)}. B·ªè qua: {skipped_count} (do l·ªói map v·ªã tr√≠).\")\n",
    "    return Dataset.from_list(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7291de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Loading Pipeline ---\n",
    "\n",
    "FILE_PATH = '/content/train.json' # File json g·ªëc\n",
    "\n",
    "# Load JSON v·ªõi x·ª≠ l√Ω l·ªói escape\n",
    "json_data = load_json_robust(FILE_PATH)\n",
    "data_list = json_data['data'] if 'data' in json_data else json_data\n",
    "\n",
    "print(f\"üìä T·ªïng s·ªë m·∫´u: {len(data_list)}\")\n",
    "\n",
    "# Ch·∫°y Preprocess (Segmentation)\n",
    "dataset = preprocess_dataset_with_segmentation(data_list)\n",
    "\n",
    "# Chia Train/Test\n",
    "datasets_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "print(f\"üìä Train: {len(datasets_split['train'])}, Test: {len(datasets_split['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. TOKENIZATION & TRAINING\n",
    "# ==========================================\n",
    "\n",
    "MODEL_CHECKPOINT = \"vinai/phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "MAX_LENGTH = 256\n",
    "DOC_STRIDE = 128\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    examples[\"question\"] = [q.strip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "print(\"üîÑ ƒêang tokenize dataset...\")\n",
    "tokenized_datasets = datasets_split.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets_split[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenize xong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c92ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Trainer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./phobert-viquad-vncorenlp\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa17aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. TEST D·ª∞ ƒêO√ÅN (C√ì T√ÅCH T·ª™)\n",
    "# ==========================================\n",
    "def predict_answer_robust(context, question):\n",
    "    # Quan tr·ªçng: Ph·∫£i t√°ch t·ª´ c√¢u h·ªèi v√† context tr∆∞·ªõc khi ƒë∆∞a v√†o model\n",
    "    # v√¨ model ƒë√£ ƒë∆∞·ª£c train tr√™n d·ªØ li·ªáu ƒë√£ t√°ch t·ª´\n",
    "    context_seg = segment_text(context)\n",
    "    question_seg = segment_text(question)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        question_seg, \n",
    "        context_seg, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=\"only_second\",\n",
    "        max_length=256\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    start_idx = torch.argmax(outputs.start_logits)\n",
    "    end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "    \n",
    "    pred_tokens = inputs.input_ids[0, start_idx:end_idx]\n",
    "    answer = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Optional: Thay th·∫ø d·∫•u _ b·∫±ng kho·∫£ng tr·∫Øng ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "    return answer.replace(\"_\", \" \")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"TEST D·ª∞ ƒêO√ÅN\")\n",
    "sample_test = \"H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa n∆∞·ªõc C·ªông h√≤a X√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam.\"\n",
    "question_test = \"Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam l√† g√¨?\"\n",
    "\n",
    "print(f\"Context: {sample_test}\")\n",
    "print(f\"Question: {question_test}\")\n",
    "print(f\"Prediction: {predict_answer_robust(sample_test, question_test)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
