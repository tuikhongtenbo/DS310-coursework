{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14377126,"sourceType":"datasetVersion","datasetId":9181544},{"sourceId":708947,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":538448,"modelId":551736}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -e . -q\n!pip install -q datasets evaluate\n!pip install wandb weave","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:49:36.243022Z","iopub.execute_input":"2026-01-05T12:49:36.243305Z","iopub.status.idle":"2026-01-05T12:50:02.962771Z","shell.execute_reply.started":"2026-01-05T12:49:36.243277Z","shell.execute_reply":"2026-01-05T12:50:02.962096Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: file:///kaggle/working does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\nCollecting weave\n  Downloading weave-0.52.22-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.5)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\nCollecting diskcache==5.6.3 (from weave)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nCollecting gql>=3.0.0 (from gql[httpx]>=3.0.0->weave)\n  Downloading gql-4.0.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: jsonschema>=4.23.0 in /usr/local/lib/python3.12/dist-packages (from weave) (4.25.1)\nCollecting polyfile-weave (from weave)\n  Downloading polyfile_weave-0.5.7-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/local/lib/python3.12/dist-packages (from weave) (8.5.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nCollecting graphql-core<3.3,>=3.2 (from gql>=3.0.0->gql[httpx]>=3.0.0->weave)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.12/dist-packages (from gql>=3.0.0->gql[httpx]>=3.0.0->weave) (1.22.0)\nCollecting backoff<3.0,>=1.11.1 (from gql>=3.0.0->gql[httpx]>=3.0.0->weave)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gql>=3.0.0->gql[httpx]>=3.0.0->weave) (4.12.0)\nRequirement already satisfied: httpx<1,>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from gql[httpx]>=3.0.0->weave) (0.28.1)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (25.4.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (0.27.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\nCollecting abnf~=2.2.0 (from polyfile-weave->weave)\n  Downloading abnf-2.2.0-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: chardet>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (5.2.0)\nCollecting cint>=1.0.0 (from polyfile-weave->weave)\n  Downloading cint-1.0.0-py3-none-any.whl.metadata (511 bytes)\nCollecting fickling>=0.0.8 (from polyfile-weave->weave)\n  Downloading fickling-0.1.6-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: graphviz>=0.20.1 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (0.21)\nCollecting intervaltree>=2.4.0 (from polyfile-weave->weave)\n  Downloading intervaltree-3.2.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: jinja2>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (3.1.6)\nCollecting kaitaistruct~=0.10 (from polyfile-weave->weave)\n  Downloading kaitaistruct-0.11-py2.py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: networkx>=2.6.3 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (3.5)\nCollecting pdfminer.six<=20250506,>=20220524 (from polyfile-weave->weave)\n  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (11.3.0)\nCollecting setuptools>=80.9.0 (from polyfile-weave->weave)\n  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\nCollecting stdlib-list~=0.11.1 (from fickling>=0.0.8->polyfile-weave->weave)\n  Downloading stdlib_list-0.11.1-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->gql[httpx]>=3.0.0->weave) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.27.0->gql[httpx]>=3.0.0->weave) (0.16.0)\nRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from intervaltree>=2.4.0->polyfile-weave->weave) (2.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.1.0->polyfile-weave->weave) (3.0.3)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (46.0.3)\nRequirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql>=3.0.0->gql[httpx]>=3.0.0->weave) (6.7.0)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql>=3.0.0->gql[httpx]>=3.0.0->weave) (0.4.1)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (2.23)\nDownloading weave-0.52.22-py3-none-any.whl (788 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gql-4.0.0-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading polyfile_weave-0.5.7-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading abnf-2.2.0-py3-none-any.whl (39 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading cint-1.0.0-py3-none-any.whl (5.6 kB)\nDownloading fickling-0.1.6-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading intervaltree-3.2.1-py2.py3-none-any.whl (25 kB)\nDownloading kaitaistruct-0.11-py2.py3-none-any.whl (11 kB)\nDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading stdlib_list-0.11.1-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: stdlib-list, setuptools, kaitaistruct, intervaltree, graphql-core, diskcache, cint, backoff, abnf, gql, fickling, pdfminer.six, polyfile-weave, weave\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.2.0\n    Uninstalling setuptools-75.2.0:\n      Successfully uninstalled setuptools-75.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.2.19 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed abnf-2.2.0 backoff-2.2.1 cint-1.0.0 diskcache-5.6.3 fickling-0.1.6 gql-4.0.0 graphql-core-3.2.7 intervaltree-3.2.1 kaitaistruct-0.11 pdfminer.six-20250506 polyfile-weave-0.5.7 setuptools-80.9.0 stdlib-list-0.11.1 weave-0.52.22\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://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@github.com/tuikhongtenbo/DS310","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:50:02.964548Z","iopub.execute_input":"2026-01-05T12:50:02.964811Z","iopub.status.idle":"2026-01-05T12:50:05.900365Z","shell.execute_reply.started":"2026-01-05T12:50:02.964785Z","shell.execute_reply":"2026-01-05T12:50:05.899483Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Cloning into 'DS310'...\nremote: Enumerating objects: 179, done.\u001b[K\nremote: Counting objects: 100% (179/179), done.\u001b[K\nremote: Compressing objects: 100% (125/125), done.\u001b[K\nremote: Total 179 (delta 94), reused 132 (delta 50), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (179/179), 22.19 MiB | 21.00 MiB/s, done.\nResolving deltas: 100% (94/94), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%cd /kaggle/working/DS310/TH3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:50:05.901660Z","iopub.execute_input":"2026-01-05T12:50:05.902017Z","iopub.status.idle":"2026-01-05T12:50:05.908490Z","shell.execute_reply.started":"2026-01-05T12:50:05.901989Z","shell.execute_reply":"2026-01-05T12:50:05.907761Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/DS310/TH3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"DS310-ViQuAD2-QA\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:50:05.909420Z","iopub.execute_input":"2026-01-05T12:50:05.909730Z","iopub.status.idle":"2026-01-05T12:50:06.197063Z","shell.execute_reply.started":"2026-01-05T12:50:05.909687Z","shell.execute_reply":"2026-01-05T12:50:06.196486Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# mBERT finetune","metadata":{}},{"cell_type":"code","source":"MODEL_NAME=\"google-bert/bert-base-multilingual-cased\"\nOUTPUT_DIR=\"/kaggle/working/mbert-base\"\n\n!python run_qa.py \\\n  --model_name_or_path $MODEL_NAME \\\n  --train_file \"/kaggle/input/uit-viquad-2-0/train.json\" \\\n  --validation_file \"/kaggle/input/uit-viquad-2-0/dev.json\" \\\n  --do_train \\\n  --do_eval \\\n  --eval_strategy steps \\\n  --save_strategy steps \\\n  --per_device_train_batch_size 32 \\\n  --per_device_eval_batch_size 12 \\\n  --gradient_accumulation_steps 5 \\\n  --learning_rate 1e-4 \\\n  --num_train_epochs 2 \\\n  --warmup_ratio 0.1 \\\n  --max_seq_length 314 \\\n  --doc_stride 128 \\\n  --fp16 \\\n  --dataloader_num_workers 4 \\\n  --logging_steps 100 \\\n  --save_steps 500 \\\n  --eval_steps 500 \\\n  --load_best_model_at_end \\\n  --metric_for_best_model f1 \\\n  --greater_is_better true \\\n  --save_total_limit 3 \\\n  --seed 53 \\\n  --weight_decay 0.01 \\\n  --adam_epsilon 1e-8 \\\n  --max_grad_norm 1.0 \\\n  --output_dir $OUTPUT_DIR \\\n  --version_2_with_negative \\\n  --n_best_size 20 \\\n  --max_answer_length 30 \\\n  --null_score_diff_threshold 0.0 \\\n  --overwrite_output_dir \\\n  --report_to wandb \\\n  --run_name \"mbert-2epoch-bs32\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:54:08.775923Z","iopub.execute_input":"2026-01-05T12:54:08.776547Z","iopub.status.idle":"2026-01-05T13:21:20.349894Z","shell.execute_reply.started":"2026-01-05T12:54:08.776517Z","shell.execute_reply":"2026-01-05T13:21:20.349063Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2026-01-05 12:54:20.455001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767617660.658674     136 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767617660.717187     136 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767617661.187875     136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767617661.187920     136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767617661.187924     136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767617661.187928     136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nWARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\nINFO:__main__:Training/evaluation parameters TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=True,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=4,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=500,\neval_strategy=IntervalStrategy.STEPS,\neval_use_gather_object=False,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=5,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=None,\nhub_revision=None,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=no,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0001,\nlength_column_name=length,\nliger_kernel_config=None,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=/kaggle/working/mbert-base/runs/Jan05_12-54-38_208f42cc63ac,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=100,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_kwargs={},\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=f1,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=2.0,\noptim=OptimizerNames.ADAMW_TORCH_FUSED,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=/kaggle/working/mbert-base,\noverwrite_output_dir=True,\nparallelism_config=None,\npast_index=-1,\nper_device_eval_batch_size=12,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\nproject=huggingface,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=mbert-2epoch-bs32,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=SaveStrategy.STEPS,\nsave_total_limit=3,\nseed=53,\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\ntrackio_space_id=trackio,\nuse_cpu=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\n)\nUsing custom data configuration default-58f204e95657dc1b\nINFO:datasets.builder:Using custom data configuration default-58f204e95657dc1b\nGenerating dataset json (/root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nINFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nINFO:datasets.builder:Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nDownloading took 0.0 min\nINFO:datasets.download.download_manager:Downloading took 0.0 min\nChecksum Computation took 0.0 min\nINFO:datasets.download.download_manager:Checksum Computation took 0.0 min\nGenerating train split\nINFO:datasets.builder:Generating train split\nGenerating train split: 22765 examples [00:02, 10921.88 examples/s]\nGenerating validation split\nINFO:datasets.builder:Generating validation split\nGenerating validation split: 5692 examples [00:00, 7773.36 examples/s]\nUnable to verify splits sizes.\nINFO:datasets.utils.info_utils:Unable to verify splits sizes.\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\nINFO:datasets.builder:Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\nconfig.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:00<00:00, 4.15MB/s]\n[INFO|configuration_utils.py:765] 2026-01-05 12:54:42,377 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 12:54:42,379 >> Model config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\ntokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.0/49.0 [00:00<00:00, 410kB/s]\n[INFO|configuration_utils.py:765] 2026-01-05 12:54:42,590 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 12:54:42,591 >> Model config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nvocab.txt: 996kB [00:00, 69.2MB/s]\ntokenizer.json: 1.96MB [00:00, 139MB/s]\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 12:54:44,021 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 12:54:44,021 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 12:54:44,021 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 12:54:44,021 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 12:54:44,021 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 12:54:44,021 >> loading file chat_template.jinja from cache at None\n[INFO|configuration_utils.py:765] 2026-01-05 12:54:44,022 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 12:54:44,022 >> Model config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nmodel.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 714M/714M [00:05<00:00, 134MB/s]\n[INFO|modeling_utils.py:1172] 2026-01-05 12:54:50,255 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-bert--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n[INFO|logging.py:343] 2026-01-05 12:54:50,281 >> A pretrained model of type `BertForQuestionAnswering` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n* `cls.predictions.transform.LayerNorm.beta` -> `cls.predictions.transform.LayerNorm.bias`\n* `cls.predictions.transform.LayerNorm.gamma` -> `cls.predictions.transform.LayerNorm.weight`\nIf you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n[INFO|modeling_utils.py:5525] 2026-01-05 12:54:50,332 >> Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:5535] 2026-01-05 12:54:50,332 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nRunning tokenizer on train dataset:   0%|      | 0/22765 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-0ad45e84fade2937.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-0ad45e84fade2937.arrow\nRunning tokenizer on train dataset: 100%|â–ˆ| 22765/22765 [00:13<00:00, 1721.35 ex\nRunning tokenizer on validation dataset:   0%|  | 0/5692 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-738783910163658b.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-58f204e95657dc1b/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-738783910163658b.arrow\nRunning tokenizer on validation dataset: 100%|â–ˆ| 5692/5692 [00:04<00:00, 1375.34\nDownloading builder script: 6.47kB [00:00, 18.6MB/s]\nDownloading extra modules: 11.3kB [00:00, 32.7MB/s]\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n[INFO|trainer.py:749] 2026-01-05 12:55:12,642 >> Using auto half precision backend\n[INFO|trainer.py:2519] 2026-01-05 12:55:13,064 >> ***** Running training *****\n[INFO|trainer.py:2520] 2026-01-05 12:55:13,064 >>   Num examples = 26,775\n[INFO|trainer.py:2521] 2026-01-05 12:55:13,064 >>   Num Epochs = 2\n[INFO|trainer.py:2522] 2026-01-05 12:55:13,064 >>   Instantaneous batch size per device = 32\n[INFO|trainer.py:2525] 2026-01-05 12:55:13,064 >>   Total train batch size (w. parallel, distributed & accumulation) = 160\n[INFO|trainer.py:2526] 2026-01-05 12:55:13,064 >>   Gradient Accumulation steps = 5\n[INFO|trainer.py:2527] 2026-01-05 12:55:13,064 >>   Total optimization steps = 336\n[INFO|trainer.py:2528] 2026-01-05 12:55:13,065 >>   Number of trainable parameters = 177,264,386\n[INFO|integration_utils.py:867] 2026-01-05 12:55:13,065 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mply58509\u001b[0m (\u001b[33mply58509-uit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run qy2ewjou (0.2s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run qy2ewjou (0.2s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/DS310/TH3/wandb/run-20260105_125513-qy2ewjou\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmbert-2epoch-bs32\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ply58509-uit/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ply58509-uit/huggingface/runs/qy2ewjou\u001b[0m\n  0%|                                                   | 0/336 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{'loss': 2.6132, 'grad_norm': 7.344239711761475, 'learning_rate': 7.847682119205298e-05, 'epoch': 0.6}\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 168/336 [12:14<09:54,  3.54s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{'loss': 1.4575, 'grad_norm': 6.982288837432861, 'learning_rate': 4.536423841059603e-05, 'epoch': 1.19}\n{'loss': 1.1717, 'grad_norm': 5.7931718826293945, 'learning_rate': 1.2251655629139073e-05, 'epoch': 1.79}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [24:28<00:00,  3.55s/it][INFO|trainer.py:4309] 2026-01-05 13:19:43,917 >> Saving model checkpoint to /kaggle/working/mbert-base/checkpoint-336\n[INFO|configuration_utils.py:491] 2026-01-05 13:19:43,919 >> Configuration saved in /kaggle/working/mbert-base/checkpoint-336/config.json\n[INFO|modeling_utils.py:4181] 2026-01-05 13:19:45,311 >> Model weights saved in /kaggle/working/mbert-base/checkpoint-336/model.safetensors\n[INFO|tokenization_utils_base.py:2590] 2026-01-05 13:19:45,313 >> tokenizer config file saved in /kaggle/working/mbert-base/checkpoint-336/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2599] 2026-01-05 13:19:45,314 >> Special tokens file saved in /kaggle/working/mbert-base/checkpoint-336/special_tokens_map.json\n[INFO|trainer.py:2810] 2026-01-05 13:19:47,305 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 1474.2405, 'train_samples_per_second': 36.324, 'train_steps_per_second': 0.228, 'train_loss': 1.6825176647731237, 'epoch': 2.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [24:32<00:00,  4.38s/it]\n[INFO|trainer.py:4309] 2026-01-05 13:19:47,313 >> Saving model checkpoint to /kaggle/working/mbert-base\n[INFO|configuration_utils.py:491] 2026-01-05 13:19:47,315 >> Configuration saved in /kaggle/working/mbert-base/config.json\n[INFO|modeling_utils.py:4181] 2026-01-05 13:19:48,693 >> Model weights saved in /kaggle/working/mbert-base/model.safetensors\n[INFO|tokenization_utils_base.py:2590] 2026-01-05 13:19:48,694 >> tokenizer config file saved in /kaggle/working/mbert-base/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2599] 2026-01-05 13:19:48,695 >> Special tokens file saved in /kaggle/working/mbert-base/special_tokens_map.json\n***** train metrics *****\n  epoch                    =        2.0\n  total_flos               =  7991960GF\n  train_loss               =     1.6825\n  train_runtime            = 0:24:34.24\n  train_samples            =      26775\n  train_samples_per_second =     36.324\n  train_steps_per_second   =      0.228\nINFO:__main__:*** Evaluate ***\n[INFO|trainer.py:1012] 2026-01-05 13:19:48,766 >> The following columns in the Evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4643] 2026-01-05 13:19:48,772 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4645] 2026-01-05 13:19:48,773 >>   Num examples = 6669\n[INFO|trainer.py:4648] 2026-01-05 13:19:48,773 >>   Batch size = 12\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 556/556 [01:00<00:00,  8.46it/s]INFO:utils_qa:Post-processing 5692 example predictions split into 6669 features.\n\n  0%|                                                  | 0/5692 [00:00<?, ?it/s]\u001b[A\n  1%|â–                                       | 36/5692 [00:00<00:15, 355.96it/s]\u001b[A\n  1%|â–Œ                                       | 72/5692 [00:00<00:16, 341.61it/s]\u001b[A\n  2%|â–‹                                      | 107/5692 [00:00<00:16, 333.65it/s]\u001b[A\n  2%|â–‰                                      | 141/5692 [00:00<00:16, 331.25it/s]\u001b[A\n  3%|â–ˆâ–                                     | 175/5692 [00:00<00:16, 329.12it/s]\u001b[A\n  4%|â–ˆâ–                                     | 210/5692 [00:00<00:16, 334.28it/s]\u001b[A\n  4%|â–ˆâ–‹                                     | 245/5692 [00:00<00:16, 339.27it/s]\u001b[A\n  5%|â–ˆâ–‰                                     | 279/5692 [00:00<00:16, 326.07it/s]\u001b[A\n  5%|â–ˆâ–ˆâ–                                    | 313/5692 [00:00<00:16, 328.86it/s]\u001b[A\n  6%|â–ˆâ–ˆâ–                                    | 346/5692 [00:01<00:16, 320.28it/s]\u001b[A\n  7%|â–ˆâ–ˆâ–Œ                                    | 380/5692 [00:01<00:16, 324.54it/s]\u001b[A\n  7%|â–ˆâ–ˆâ–Š                                    | 417/5692 [00:01<00:15, 336.75it/s]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆ                                    | 452/5692 [00:01<00:15, 338.63it/s]\u001b[A\n  9%|â–ˆâ–ˆâ–ˆâ–                                   | 486/5692 [00:01<00:16, 325.32it/s]\u001b[A\n  9%|â–ˆâ–ˆâ–ˆâ–Œ                                   | 519/5692 [00:01<00:16, 312.41it/s]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–Š                                   | 552/5692 [00:01<00:16, 315.31it/s]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–ˆ                                   | 584/5692 [00:01<00:16, 314.79it/s]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 617/5692 [00:01<00:15, 318.23it/s]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 654/5692 [00:01<00:15, 332.77it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 688/5692 [00:02<00:14, 334.34it/s]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 725/5692 [00:02<00:14, 343.16it/s]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 760/5692 [00:02<00:14, 336.36it/s]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 794/5692 [00:02<00:14, 329.74it/s]\u001b[A\n 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 828/5692 [00:02<00:14, 330.66it/s]\u001b[A\n 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 862/5692 [00:02<00:14, 327.32it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 895/5692 [00:02<00:14, 327.04it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 929/5692 [00:02<00:14, 325.92it/s]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 963/5692 [00:02<00:14, 329.40it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 1001/5692 [00:03<00:13, 343.57it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 1039/5692 [00:03<00:13, 353.81it/s]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1075/5692 [00:03<00:13, 354.61it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1111/5692 [00:03<00:12, 353.81it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 1147/5692 [00:03<00:13, 347.58it/s]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 1182/5692 [00:03<00:13, 340.19it/s]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 1217/5692 [00:03<00:13, 333.46it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 1252/5692 [00:03<00:13, 336.98it/s]\u001b[A\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 1286/5692 [00:03<00:13, 323.47it/s]\u001b[A\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 1319/5692 [00:03<00:13, 323.48it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 1354/5692 [00:04<00:13, 328.94it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1387/5692 [00:04<00:13, 328.99it/s]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1420/5692 [00:04<00:13, 313.99it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 1454/5692 [00:04<00:13, 320.16it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 1488/5692 [00:04<00:12, 325.49it/s]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1522/5692 [00:04<00:12, 328.95it/s]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1555/5692 [00:04<00:12, 326.20it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 1591/5692 [00:04<00:12, 334.16it/s]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 1628/5692 [00:04<00:11, 339.81it/s]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 1663/5692 [00:05<00:12, 327.26it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 1696/5692 [00:05<00:12, 326.43it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 1730/5692 [00:05<00:12, 330.03it/s]\u001b[A\n 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 1766/5692 [00:05<00:11, 334.44it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 1800/5692 [00:05<00:12, 318.46it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 1833/5692 [00:05<00:12, 298.73it/s]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 1864/5692 [00:05<00:12, 298.72it/s]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 1895/5692 [00:05<00:12, 299.37it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 1926/5692 [00:05<00:12, 302.21it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 1959/5692 [00:05<00:12, 305.70it/s]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 1995/5692 [00:06<00:11, 319.57it/s]\u001b[A\n 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 2028/5692 [00:06<00:11, 317.14it/s]\u001b[A\n 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 2061/5692 [00:06<00:11, 318.16it/s]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 2093/5692 [00:06<00:11, 303.69it/s]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2125/5692 [00:06<00:11, 305.93it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2156/5692 [00:06<00:12, 287.67it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 2186/5692 [00:06<00:12, 289.43it/s]\u001b[A\n 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2221/5692 [00:06<00:11, 304.91it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 2253/5692 [00:06<00:11, 307.07it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2285/5692 [00:07<00:11, 307.00it/s]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2318/5692 [00:07<00:10, 313.63it/s]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 2350/5692 [00:07<00:10, 315.42it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 2382/5692 [00:07<00:10, 304.64it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 2413/5692 [00:07<00:10, 302.71it/s]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2448/5692 [00:07<00:10, 315.36it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 2480/5692 [00:07<00:10, 313.72it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 2516/5692 [00:07<00:09, 322.91it/s]\u001b[A\n 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 2553/5692 [00:07<00:09, 334.60it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2590/5692 [00:07<00:09, 344.33it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 2625/5692 [00:08<00:08, 343.69it/s]\u001b[A\n 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 2660/5692 [00:08<00:09, 336.54it/s]\u001b[A\n 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 2694/5692 [00:08<00:09, 321.89it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 2729/5692 [00:08<00:09, 328.92it/s]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 2764/5692 [00:08<00:08, 333.42it/s]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 2801/5692 [00:08<00:08, 339.76it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 2836/5692 [00:08<00:08, 331.82it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 2874/5692 [00:08<00:08, 344.65it/s]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 2909/5692 [00:08<00:08, 344.06it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 2944/5692 [00:09<00:08, 340.45it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 2979/5692 [00:09<00:08, 329.47it/s]\u001b[A\n 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 3013/5692 [00:09<00:08, 330.41it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 3047/5692 [00:09<00:08, 327.04it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 3080/5692 [00:09<00:08, 326.36it/s]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 3115/5692 [00:09<00:07, 333.20it/s]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 3151/5692 [00:09<00:07, 338.62it/s]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 3185/5692 [00:09<00:07, 338.09it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 3220/5692 [00:09<00:07, 340.12it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 3255/5692 [00:09<00:07, 335.69it/s]\u001b[A\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 3289/5692 [00:10<00:07, 334.43it/s]\u001b[A\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 3323/5692 [00:10<00:07, 331.97it/s]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 3357/5692 [00:10<00:07, 330.42it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 3391/5692 [00:10<00:07, 309.85it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 3425/5692 [00:10<00:07, 315.77it/s]\u001b[A\n 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 3457/5692 [00:10<00:07, 310.38it/s]\u001b[A\n 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 3489/5692 [00:10<00:07, 310.32it/s]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 3523/5692 [00:10<00:06, 317.89it/s]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 3560/5692 [00:10<00:06, 331.40it/s]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 3595/5692 [00:11<00:06, 334.53it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3629/5692 [00:11<00:06, 328.92it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3662/5692 [00:11<00:06, 326.14it/s]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 3695/5692 [00:11<00:06, 326.26it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 3731/5692 [00:11<00:05, 334.97it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3767/5692 [00:11<00:05, 339.34it/s]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3802/5692 [00:11<00:05, 339.83it/s]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 3836/5692 [00:11<00:05, 328.92it/s]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 3875/5692 [00:11<00:05, 346.31it/s]\u001b[A\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 3910/5692 [00:11<00:05, 345.07it/s]\u001b[A\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 3947/5692 [00:12<00:04, 352.11it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 3983/5692 [00:12<00:04, 349.47it/s]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 4020/5692 [00:12<00:04, 355.40it/s]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 4056/5692 [00:12<00:04, 352.55it/s]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 4094/5692 [00:12<00:04, 358.47it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 4130/5692 [00:12<00:04, 347.83it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 4165/5692 [00:12<00:04, 342.88it/s]\u001b[A\n 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 4200/5692 [00:12<00:04, 325.85it/s]\u001b[A\n 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 4233/5692 [00:12<00:04, 325.44it/s]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 4269/5692 [00:13<00:04, 333.83it/s]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 4306/5692 [00:13<00:04, 344.23it/s]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 4342/5692 [00:13<00:03, 347.46it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 4377/5692 [00:13<00:04, 325.84it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 4411/5692 [00:13<00:03, 325.11it/s]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 4444/5692 [00:13<00:03, 322.57it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 4481/5692 [00:13<00:03, 333.75it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 4519/5692 [00:13<00:03, 345.65it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 4554/5692 [00:13<00:03, 336.37it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 4591/5692 [00:13<00:03, 337.73it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 4627/5692 [00:14<00:03, 343.71it/s]\u001b[A\n 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 4662/5692 [00:14<00:03, 340.69it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 4697/5692 [00:14<00:02, 333.25it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 4731/5692 [00:14<00:02, 327.76it/s]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 4765/5692 [00:14<00:02, 329.75it/s]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 4799/5692 [00:14<00:02, 327.34it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 4832/5692 [00:14<00:02, 321.97it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 4865/5692 [00:14<00:02, 324.15it/s]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4899/5692 [00:14<00:02, 324.88it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4935/5692 [00:15<00:02, 331.13it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4969/5692 [00:15<00:02, 330.08it/s]\u001b[A\n 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5008/5692 [00:15<00:01, 343.07it/s]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5045/5692 [00:15<00:01, 349.15it/s]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5080/5692 [00:15<00:01, 344.47it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5115/5692 [00:15<00:01, 321.25it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5149/5692 [00:15<00:01, 325.14it/s]\u001b[A\n 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 5183/5692 [00:15<00:01, 327.88it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 5219/5692 [00:15<00:01, 334.39it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 5253/5692 [00:15<00:01, 335.47it/s]\u001b[A\n 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5288/5692 [00:16<00:01, 335.88it/s]\u001b[A\n 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 5322/5692 [00:16<00:01, 321.10it/s]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 5355/5692 [00:16<00:01, 315.78it/s]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 5387/5692 [00:16<00:00, 315.74it/s]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5419/5692 [00:16<00:00, 313.18it/s]\u001b[A\n 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5453/5692 [00:16<00:00, 319.14it/s]\u001b[A\n 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5489/5692 [00:16<00:00, 329.82it/s]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5523/5692 [00:16<00:00, 329.17it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5557/5692 [00:16<00:00, 330.27it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5591/5692 [00:17<00:00, 332.94it/s]\u001b[A\n 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5625/5692 [00:17<00:00, 333.07it/s]\u001b[A\n 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5659/5692 [00:17<00:00, 325.03it/s]\u001b[A\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5692/5692 [00:17<00:00, 328.49it/s]\u001b[A\nINFO:utils_qa:Saving predictions to /kaggle/working/mbert-base/eval_predictions.json.\nINFO:utils_qa:Saving nbest_preds to /kaggle/working/mbert-base/eval_nbest_predictions.json.\nINFO:utils_qa:Saving null_odds to /kaggle/working/mbert-base/eval_null_odds.json.\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 556/556 [01:26<00:00,  6.44it/s]\n***** eval metrics *****\n  epoch                   =        2.0\n  eval_HasAns_exact       =    39.3302\n  eval_HasAns_f1          =    59.5774\n  eval_HasAns_total       =       3852\n  eval_NoAns_exact        =    54.0217\n  eval_NoAns_f1           =    54.0217\n  eval_NoAns_total        =       1840\n  eval_best_exact         =    44.1145\n  eval_best_exact_thresh  =        0.0\n  eval_best_f1            =    57.8166\n  eval_best_f1_thresh     =        0.0\n  eval_exact              =    44.0794\n  eval_f1                 =    57.7814\n  eval_runtime            = 0:01:00.86\n  eval_samples            =       6669\n  eval_samples_per_second =    109.564\n  eval_steps_per_second   =      9.134\n  eval_total              =       5692\n[INFO|modelcard.py:456] 2026-01-05 13:21:15,660 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: ğŸš€ View run \u001b[33mmbert-2epoch-bs32\u001b[0m at: \u001b[34m\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20260105_125513-qy2ewjou/logs\u001b[0m\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"OUTPUT_DIR=\"/kaggle/working/mbert-base\"\n\n!python run_qa.py \\\n  --model_name_or_path $OUTPUT_DIR \\\n  --test_file \"/kaggle/input/uit-viquad-2-0/Private_Test_ref.json\" \\\n  --do_predict \\\n  --per_device_eval_batch_size 16 \\\n  --max_seq_length 512 \\\n  --doc_stride 128 \\\n  --dataloader_num_workers 4 \\\n  --output_dir $OUTPUT_DIR \\\n  --version_2_with_negative \\\n  --n_best_size 20 \\\n  --max_answer_length 30 \\\n  --null_score_diff_threshold 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:23:43.311352Z","iopub.execute_input":"2026-01-05T13:23:43.311702Z","iopub.status.idle":"2026-01-05T13:26:37.383467Z","shell.execute_reply.started":"2026-01-05T13:23:43.311671Z","shell.execute_reply":"2026-01-05T13:26:37.382601Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2026-01-05 13:23:47.835695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767619427.857605     274 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767619427.864365     274 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767619427.881462     274 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767619427.881492     274 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767619427.881495     274 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767619427.881499     274 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nWARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\nINFO:__main__:Training/evaluation parameters TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=True,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=4,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=False,\ndo_predict=True,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=None,\neval_strategy=IntervalStrategy.NO,\neval_use_gather_object=False,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=None,\nhub_revision=None,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=no,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nliger_kernel_config=None,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=/kaggle/working/mbert-base/runs/Jan05_13-23-53_208f42cc63ac,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_kwargs={},\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=OptimizerNames.ADAMW_TORCH_FUSED,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=/kaggle/working/mbert-base,\noverwrite_output_dir=False,\nparallelism_config=None,\npast_index=-1,\nper_device_eval_batch_size=16,\nper_device_train_batch_size=8,\nprediction_loss_only=False,\nproject=huggingface,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=None,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=SaveStrategy.STEPS,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\ntrackio_space_id=trackio,\nuse_cpu=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.0,\n)\nUsing custom data configuration default-08990d13f0a33eac\nINFO:datasets.builder:Using custom data configuration default-08990d13f0a33eac\nGenerating dataset json (/root/.cache/huggingface/datasets/json/default-08990d13f0a33eac/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nINFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-08990d13f0a33eac/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-08990d13f0a33eac/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nINFO:datasets.builder:Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-08990d13f0a33eac/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nDownloading took 0.0 min\nINFO:datasets.download.download_manager:Downloading took 0.0 min\nChecksum Computation took 0.0 min\nINFO:datasets.download.download_manager:Checksum Computation took 0.0 min\nGenerating test split\nINFO:datasets.builder:Generating test split\nGenerating test split: 7301 examples [00:00, 16304.79 examples/s]\nUnable to verify splits sizes.\nINFO:datasets.utils.info_utils:Unable to verify splits sizes.\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-08990d13f0a33eac/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\nINFO:datasets.builder:Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-08990d13f0a33eac/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\n[INFO|configuration_utils.py:763] 2026-01-05 13:23:55,848 >> loading configuration file /kaggle/working/mbert-base/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 13:23:55,850 >> Model config BertConfig {\n  \"architectures\": [\n    \"BertForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"dtype\": \"float32\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 13:23:55,856 >> loading file vocab.txt\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 13:23:55,856 >> loading file tokenizer.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 13:23:55,856 >> loading file added_tokens.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 13:23:55,856 >> loading file special_tokens_map.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 13:23:55,856 >> loading file tokenizer_config.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 13:23:55,856 >> loading file chat_template.jinja\n[INFO|modeling_utils.py:1169] 2026-01-05 13:23:55,989 >> loading weights file /kaggle/working/mbert-base/model.safetensors\nRunning tokenizer on prediction dataset:   0%|  | 0/7301 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-08990d13f0a33eac/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-6a09ce06f2240b71.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-08990d13f0a33eac/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-6a09ce06f2240b71.arrow\nRunning tokenizer on prediction dataset: 100%|â–ˆ| 7301/7301 [00:06<00:00, 1197.27\nINFO:__main__:*** Predict ***\n[INFO|trainer.py:1012] 2026-01-05 13:24:04,127 >> The following columns in the test set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4643] 2026-01-05 13:24:04,137 >> \n***** Running Prediction *****\n[INFO|trainer.py:4645] 2026-01-05 13:24:04,137 >>   Num examples = 7387\n[INFO|trainer.py:4648] 2026-01-05 13:24:04,138 >>   Batch size = 16\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462/462 [01:51<00:00,  4.25it/s]INFO:utils_qa:Post-processing 7301 example predictions split into 7387 features.\n\n  0%|                                                  | 0/7301 [00:00<?, ?it/s]\u001b[A\n  0%|â–                                       | 29/7301 [00:00<00:25, 284.08it/s]\u001b[A\n  1%|â–                                       | 59/7301 [00:00<00:24, 293.19it/s]\u001b[A\n  1%|â–                                       | 89/7301 [00:00<00:24, 296.03it/s]\u001b[A\n  2%|â–‹                                      | 119/7301 [00:00<00:24, 295.37it/s]\u001b[A\n  2%|â–Š                                      | 149/7301 [00:00<00:24, 293.67it/s]\u001b[A\n  2%|â–‰                                      | 179/7301 [00:00<00:25, 276.22it/s]\u001b[A\n  3%|â–ˆ                                      | 207/7301 [00:00<00:32, 220.47it/s]\u001b[A\n  3%|â–ˆâ–                                     | 231/7301 [00:00<00:32, 215.01it/s]\u001b[A\n  3%|â–ˆâ–                                     | 254/7301 [00:01<00:38, 184.04it/s]\u001b[A\n  4%|â–ˆâ–Œ                                     | 282/7301 [00:01<00:34, 206.25it/s]\u001b[A\n  4%|â–ˆâ–‹                                     | 311/7301 [00:01<00:30, 226.57it/s]\u001b[A\n  5%|â–ˆâ–Š                                     | 339/7301 [00:01<00:28, 240.49it/s]\u001b[A\n  5%|â–ˆâ–‰                                     | 370/7301 [00:01<00:26, 257.50it/s]\u001b[A\n  5%|â–ˆâ–ˆâ–                                    | 399/7301 [00:01<00:25, 265.95it/s]\u001b[A\n  6%|â–ˆâ–ˆâ–                                    | 429/7301 [00:01<00:25, 273.78it/s]\u001b[A\n  6%|â–ˆâ–ˆâ–                                    | 459/7301 [00:01<00:24, 279.94it/s]\u001b[A\n  7%|â–ˆâ–ˆâ–Œ                                    | 490/7301 [00:01<00:23, 286.89it/s]\u001b[A\n  7%|â–ˆâ–ˆâ–Š                                    | 520/7301 [00:02<00:23, 289.72it/s]\u001b[A\n  8%|â–ˆâ–ˆâ–‰                                    | 550/7301 [00:02<00:23, 289.66it/s]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆ                                    | 580/7301 [00:02<00:23, 290.98it/s]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆâ–                                   | 610/7301 [00:02<00:23, 288.26it/s]\u001b[A\n  9%|â–ˆâ–ˆâ–ˆâ–                                   | 640/7301 [00:02<00:23, 289.38it/s]\u001b[A\n  9%|â–ˆâ–ˆâ–ˆâ–Œ                                   | 670/7301 [00:02<00:22, 291.42it/s]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–‹                                   | 700/7301 [00:02<00:22, 291.24it/s]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–‰                                   | 730/7301 [00:02<00:22, 288.58it/s]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–ˆ                                   | 759/7301 [00:02<00:22, 286.99it/s]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 788/7301 [00:02<00:22, 287.19it/s]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 818/7301 [00:03<00:22, 288.77it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 847/7301 [00:03<00:22, 282.43it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 877/7301 [00:03<00:22, 286.45it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 907/7301 [00:03<00:22, 287.70it/s]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 936/7301 [00:03<00:22, 286.75it/s]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 966/7301 [00:03<00:21, 289.07it/s]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 995/7301 [00:03<00:21, 288.35it/s]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 1024/7301 [00:03<00:21, 286.05it/s]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 1053/7301 [00:03<00:22, 283.82it/s]\u001b[A\n 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 1082/7301 [00:03<00:21, 284.77it/s]\u001b[A\n 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 1111/7301 [00:04<00:21, 284.72it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 1140/7301 [00:04<00:21, 281.80it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 1170/7301 [00:04<00:21, 284.83it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 1199/7301 [00:04<00:21, 284.51it/s]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 1228/7301 [00:04<00:21, 282.56it/s]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 1258/7301 [00:04<00:21, 286.59it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 1287/7301 [00:04<00:21, 283.53it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 1316/7301 [00:04<00:21, 276.81it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 1344/7301 [00:04<00:22, 265.51it/s]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1374/7301 [00:05<00:21, 273.24it/s]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1404/7301 [00:05<00:21, 279.95it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1434/7301 [00:05<00:20, 283.55it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 1463/7301 [00:05<00:21, 271.90it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 1491/7301 [00:05<00:21, 265.92it/s]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 1519/7301 [00:05<00:21, 269.50it/s]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 1547/7301 [00:05<00:21, 271.76it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 1576/7301 [00:05<00:20, 275.15it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 1606/7301 [00:05<00:20, 281.60it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 1636/7301 [00:05<00:19, 286.05it/s]\u001b[A\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 1666/7301 [00:06<00:19, 288.27it/s]\u001b[A\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 1696/7301 [00:06<00:19, 289.95it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 1726/7301 [00:06<00:19, 288.97it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1755/7301 [00:06<00:19, 288.74it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1784/7301 [00:06<00:19, 287.26it/s]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1813/7301 [00:06<00:19, 287.20it/s]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 1843/7301 [00:06<00:18, 288.09it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 1872/7301 [00:06<00:18, 287.61it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 1902/7301 [00:06<00:18, 289.39it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 1932/7301 [00:06<00:18, 291.58it/s]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1962/7301 [00:07<00:18, 291.27it/s]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1992/7301 [00:07<00:18, 285.66it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 2021/7301 [00:07<00:18, 284.31it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 2050/7301 [00:07<00:18, 283.69it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 2079/7301 [00:07<00:18, 281.58it/s]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 2108/7301 [00:07<00:18, 281.64it/s]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 2137/7301 [00:07<00:18, 280.70it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 2166/7301 [00:07<00:18, 282.87it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 2195/7301 [00:07<00:18, 283.41it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2224/7301 [00:08<00:17, 283.41it/s]\u001b[A\n 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 2253/7301 [00:08<00:17, 284.86it/s]\u001b[A\n 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 2282/7301 [00:08<00:17, 285.05it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 2311/7301 [00:08<00:17, 283.86it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2340/7301 [00:08<00:17, 281.24it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2370/7301 [00:08<00:17, 284.31it/s]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2400/7301 [00:08<00:17, 287.02it/s]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 2430/7301 [00:08<00:16, 288.07it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 2459/7301 [00:08<00:16, 286.62it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 2488/7301 [00:08<00:16, 285.19it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 2517/7301 [00:09<00:16, 285.86it/s]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2547/7301 [00:09<00:16, 287.22it/s]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2576/7301 [00:09<00:16, 285.43it/s]\u001b[A\n 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 2605/7301 [00:09<00:17, 274.67it/s]\u001b[A\n 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 2633/7301 [00:09<00:17, 271.00it/s]\u001b[A\n 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 2662/7301 [00:09<00:16, 274.50it/s]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2691/7301 [00:09<00:16, 278.83it/s]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2720/7301 [00:09<00:16, 279.68it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2749/7301 [00:09<00:16, 280.12it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2778/7301 [00:09<00:16, 279.30it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 2806/7301 [00:10<00:16, 279.07it/s]\u001b[A\n 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2834/7301 [00:10<00:16, 278.33it/s]\u001b[A\n 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 2864/7301 [00:10<00:15, 282.24it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 2894/7301 [00:10<00:15, 286.22it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2923/7301 [00:10<00:15, 284.20it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2952/7301 [00:10<00:15, 284.91it/s]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2981/7301 [00:10<00:15, 283.49it/s]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 3010/7301 [00:10<00:15, 284.30it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 3039/7301 [00:10<00:14, 284.97it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 3068/7301 [00:11<00:14, 282.92it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 3098/7301 [00:11<00:14, 287.34it/s]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 3127/7301 [00:11<00:14, 285.59it/s]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 3156/7301 [00:11<00:14, 284.30it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 3185/7301 [00:11<00:14, 279.72it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 3213/7301 [00:11<00:14, 277.62it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 3243/7301 [00:11<00:14, 282.37it/s]\u001b[A\n 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3272/7301 [00:11<00:14, 279.65it/s]\u001b[A\n 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 3301/7301 [00:11<00:14, 282.22it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 3330/7301 [00:11<00:15, 262.50it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 3358/7301 [00:12<00:14, 265.81it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 3387/7301 [00:12<00:14, 271.01it/s]\u001b[A\n 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 3416/7301 [00:12<00:14, 273.98it/s]\u001b[A\n 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 3444/7301 [00:12<00:14, 270.67it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 3472/7301 [00:12<00:14, 271.42it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 3502/7301 [00:12<00:13, 277.22it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 3531/7301 [00:12<00:13, 278.34it/s]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 3559/7301 [00:12<00:13, 276.30it/s]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 3588/7301 [00:12<00:13, 279.06it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 3616/7301 [00:12<00:13, 277.66it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 3646/7301 [00:13<00:12, 282.89it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 3675/7301 [00:13<00:12, 283.41it/s]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 3704/7301 [00:13<00:12, 282.91it/s]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 3733/7301 [00:13<00:12, 281.40it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 3762/7301 [00:13<00:12, 280.04it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 3791/7301 [00:13<00:12, 281.06it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 3820/7301 [00:13<00:12, 283.07it/s]\u001b[A\n 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 3849/7301 [00:13<00:14, 241.13it/s]\u001b[A\n 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 3876/7301 [00:13<00:13, 248.51it/s]\u001b[A\n 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 3904/7301 [00:14<00:13, 256.69it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 3934/7301 [00:14<00:12, 267.57it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 3962/7301 [00:14<00:12, 269.06it/s]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 3991/7301 [00:14<00:12, 273.50it/s]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 4019/7301 [00:14<00:12, 269.62it/s]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 4049/7301 [00:14<00:11, 277.00it/s]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 4077/7301 [00:14<00:11, 273.85it/s]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 4106/7301 [00:14<00:11, 276.21it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 4134/7301 [00:14<00:11, 264.17it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 4163/7301 [00:15<00:11, 269.04it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 4191/7301 [00:15<00:11, 270.98it/s]\u001b[A\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 4221/7301 [00:15<00:11, 276.85it/s]\u001b[A\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 4251/7301 [00:15<00:10, 281.88it/s]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 4281/7301 [00:15<00:10, 286.17it/s]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 4310/7301 [00:15<00:10, 285.94it/s]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 4339/7301 [00:15<00:10, 284.93it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 4368/7301 [00:15<00:11, 252.61it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 4399/7301 [00:15<00:10, 267.14it/s]\u001b[A\n 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 4428/7301 [00:15<00:10, 272.83it/s]\u001b[A\n 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 4458/7301 [00:16<00:10, 278.97it/s]\u001b[A\n 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 4488/7301 [00:16<00:09, 282.58it/s]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 4518/7301 [00:16<00:09, 286.40it/s]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 4547/7301 [00:16<00:09, 285.81it/s]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 4577/7301 [00:16<00:09, 287.84it/s]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 4607/7301 [00:16<00:09, 288.77it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 4637/7301 [00:16<00:09, 290.47it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 4667/7301 [00:16<00:09, 290.25it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 4697/7301 [00:16<00:08, 290.98it/s]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 4727/7301 [00:17<00:08, 292.77it/s]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 4757/7301 [00:17<00:08, 294.15it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 4787/7301 [00:17<00:08, 295.47it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 4817/7301 [00:17<00:08, 295.56it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 4847/7301 [00:17<00:08, 293.43it/s]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 4877/7301 [00:17<00:08, 291.38it/s]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 4907/7301 [00:17<00:08, 291.80it/s]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 4937/7301 [00:17<00:08, 290.84it/s]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 4967/7301 [00:17<00:07, 292.86it/s]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 4997/7301 [00:17<00:07, 289.78it/s]\u001b[A\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 5026/7301 [00:18<00:07, 289.53it/s]\u001b[A\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 5056/7301 [00:18<00:07, 290.66it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 5086/7301 [00:18<00:07, 289.32it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 5115/7301 [00:18<00:07, 289.23it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 5145/7301 [00:18<00:07, 292.11it/s]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 5175/7301 [00:18<00:07, 290.59it/s]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 5205/7301 [00:18<00:07, 289.66it/s]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 5234/7301 [00:18<00:07, 283.03it/s]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 5263/7301 [00:18<00:07, 283.18it/s]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 5293/7301 [00:18<00:06, 287.47it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 5323/7301 [00:19<00:06, 288.43it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 5353/7301 [00:19<00:06, 290.56it/s]\u001b[A\n 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 5383/7301 [00:19<00:06, 291.22it/s]\u001b[A\n 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 5413/7301 [00:19<00:06, 290.76it/s]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 5443/7301 [00:19<00:06, 290.66it/s]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 5473/7301 [00:19<00:06, 286.80it/s]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 5502/7301 [00:19<00:06, 282.44it/s]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 5531/7301 [00:19<00:06, 281.65it/s]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 5560/7301 [00:19<00:06, 283.64it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 5590/7301 [00:19<00:05, 286.03it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 5619/7301 [00:20<00:05, 283.52it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 5648/7301 [00:20<00:05, 281.53it/s]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 5677/7301 [00:20<00:05, 281.60it/s]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 5706/7301 [00:20<00:05, 276.63it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 5734/7301 [00:20<00:05, 276.38it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 5763/7301 [00:20<00:05, 278.36it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 5793/7301 [00:20<00:05, 281.81it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 5822/7301 [00:20<00:05, 280.00it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 5851/7301 [00:20<00:05, 281.84it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 5881/7301 [00:21<00:04, 285.40it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 5911/7301 [00:21<00:04, 288.65it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 5941/7301 [00:21<00:04, 291.10it/s]\u001b[A\n 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 5971/7301 [00:21<00:04, 286.09it/s]\u001b[A\n 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 6000/7301 [00:21<00:04, 283.57it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 6029/7301 [00:21<00:04, 284.33it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 6058/7301 [00:21<00:04, 284.46it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 6087/7301 [00:21<00:04, 282.17it/s]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 6116/7301 [00:21<00:04, 280.47it/s]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 6146/7301 [00:21<00:04, 283.51it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 6175/7301 [00:22<00:03, 284.80it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 6204/7301 [00:22<00:03, 286.16it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 6233/7301 [00:22<00:03, 285.54it/s]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6264/7301 [00:22<00:03, 290.65it/s]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 6294/7301 [00:22<00:03, 289.57it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 6323/7301 [00:22<00:03, 286.85it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6352/7301 [00:22<00:03, 286.20it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6381/7301 [00:22<00:03, 283.15it/s]\u001b[A\n 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6411/7301 [00:22<00:03, 285.50it/s]\u001b[A\n 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 6441/7301 [00:22<00:02, 288.83it/s]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 6471/7301 [00:23<00:02, 290.02it/s]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 6501/7301 [00:23<00:02, 290.26it/s]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 6531/7301 [00:23<00:02, 288.66it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6561/7301 [00:23<00:02, 290.02it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6592/7301 [00:23<00:02, 294.42it/s]\u001b[A\n 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6622/7301 [00:23<00:02, 295.18it/s]\u001b[A\n 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6652/7301 [00:23<00:02, 296.13it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6682/7301 [00:23<00:02, 295.36it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6712/7301 [00:23<00:02, 293.25it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6742/7301 [00:24<00:01, 293.42it/s]\u001b[A\n 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6772/7301 [00:24<00:01, 291.58it/s]\u001b[A\n 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6802/7301 [00:24<00:01, 289.99it/s]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6832/7301 [00:24<00:01, 289.92it/s]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 6861/7301 [00:24<00:01, 286.86it/s]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 6890/7301 [00:24<00:01, 283.47it/s]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 6921/7301 [00:24<00:01, 289.53it/s]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6950/7301 [00:24<00:01, 276.40it/s]\u001b[A\n 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6978/7301 [00:24<00:01, 258.80it/s]\u001b[A\n 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7007/7301 [00:24<00:01, 265.15it/s]\u001b[A\n 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 7036/7301 [00:25<00:00, 271.18it/s]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7065/7301 [00:25<00:00, 276.22it/s]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 7093/7301 [00:25<00:00, 274.01it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 7122/7301 [00:25<00:00, 276.85it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7150/7301 [00:25<00:00, 276.10it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7178/7301 [00:25<00:00, 275.60it/s]\u001b[A\n 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7206/7301 [00:25<00:00, 271.68it/s]\u001b[A\n 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 7235/7301 [00:25<00:00, 274.90it/s]\u001b[A\n 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 7263/7301 [00:25<00:00, 273.23it/s]\u001b[A\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7301/7301 [00:26<00:00, 280.22it/s]\u001b[A\nINFO:utils_qa:Saving predictions to /kaggle/working/mbert-base/predict_predictions.json.\nINFO:utils_qa:Saving nbest_preds to /kaggle/working/mbert-base/predict_nbest_predictions.json.\nINFO:utils_qa:Saving null_odds to /kaggle/working/mbert-base/predict_null_odds.json.\n***** predict metrics *****\n  predict_samples             =       7387\n  test_NoAns_exact            =    33.2146\n  test_NoAns_f1               =    33.2146\n  test_NoAns_total            =       7301\n  test_best_exact             =      100.0\n  test_best_exact_thresh      =        0.0\n  test_best_f1                =      100.0\n  test_best_f1_thresh         =        0.0\n  test_exact                  =    33.2146\n  test_f1                     =    33.2146\n  test_model_preparation_time =     0.0027\n  test_runtime                = 0:01:52.27\n  test_samples_per_second     =     65.793\n  test_steps_per_second       =      4.115\n  test_total                  =       7301\n[INFO|modelcard.py:456] 2026-01-05 13:26:35,053 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462/462 [02:30<00:00,  3.07it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"OUTPUT_DIR=\"/kaggle/working/mbert-base\"\n\n!python evaluation.py \\\n  \"/kaggle/input/uit-viquad-2-0/ground_truth_private_test.json\" \\\n  \"$OUTPUT_DIR/predict_predictions.json\" \\\n  --na-prob-file \"$OUTPUT_DIR/predict_null_odds.json\" \\\n  --out-file \"$OUTPUT_DIR/evaluation_results.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:26:37.683580Z","iopub.execute_input":"2026-01-05T13:26:37.684296Z","iopub.status.idle":"2026-01-05T13:26:38.590786Z","shell.execute_reply.started":"2026-01-05T13:26:37.684265Z","shell.execute_reply":"2026-01-05T13:26:38.590095Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2026-01-05 13:26:37 - INFO - Logger initialized. Log file: ./logs/evaluation_20260105_132637.log\n2026-01-05 13:26:37 - INFO - Loading dataset from /kaggle/input/uit-viquad-2-0/ground_truth_private_test.json\n2026-01-05 13:26:37 - INFO - Loading predictions from /kaggle/working/mbert-base/predict_predictions.json\n2026-01-05 13:26:37 - INFO - Loading no-answer probabilities from /kaggle/working/mbert-base/predict_null_odds.json\n2026-01-05 13:26:37 - INFO - Dataset contains 19 examples\n2026-01-05 13:26:37 - INFO - Predictions contain 7301 entries\n2026-01-05 13:26:38 - INFO - Evaluating on 3712 questions with predictions\n2026-01-05 13:26:38 - INFO -   Has answer: 2596\n2026-01-05 13:26:38 - INFO -   No answer: 1116\n2026-01-05 13:26:38 - INFO - Evaluation Results:\n2026-01-05 13:26:38 - INFO -   Exact Match: 47.23%\n2026-01-05 13:26:38 - INFO -   F1 Score: 57.39%\n2026-01-05 13:26:38 - INFO -   HasAns - Exact: 39.87%\n2026-01-05 13:26:38 - INFO -   HasAns - F1: 54.40%\n2026-01-05 13:26:38 - INFO -   NoAns - Exact: 64.34%\n2026-01-05 13:26:38 - INFO -   NoAns - F1: 64.34%\n2026-01-05 13:26:38 - INFO - Saving evaluation results to /kaggle/working/mbert-base/evaluation_results.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# !rm -rf /kaggle/working/DS310","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-05T08:47:57.390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %cd /kaggle/working","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-05T08:47:57.390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}