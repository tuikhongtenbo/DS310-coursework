{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14377126,"sourceType":"datasetVersion","datasetId":9181544}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -e . -q\n!pip install -q datasets evaluate\n!pip install wandb weave","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:56:13.292726Z","iopub.execute_input":"2026-01-05T13:56:13.292979Z","iopub.status.idle":"2026-01-05T13:56:38.421208Z","shell.execute_reply.started":"2026-01-05T13:56:13.292950Z","shell.execute_reply":"2026-01-05T13:56:38.420524Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: file:///kaggle/working does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\nCollecting weave\n  Downloading weave-0.52.22-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.5)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\nCollecting diskcache==5.6.3 (from weave)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nCollecting gql>=3.0.0 (from gql[httpx]>=3.0.0->weave)\n  Downloading gql-4.0.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: jsonschema>=4.23.0 in /usr/local/lib/python3.12/dist-packages (from weave) (4.25.1)\nCollecting polyfile-weave (from weave)\n  Downloading polyfile_weave-0.5.7-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/local/lib/python3.12/dist-packages (from weave) (8.5.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nCollecting graphql-core<3.3,>=3.2 (from gql>=3.0.0->gql[httpx]>=3.0.0->weave)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.12/dist-packages (from gql>=3.0.0->gql[httpx]>=3.0.0->weave) (1.22.0)\nCollecting backoff<3.0,>=1.11.1 (from gql>=3.0.0->gql[httpx]>=3.0.0->weave)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gql>=3.0.0->gql[httpx]>=3.0.0->weave) (4.12.0)\nRequirement already satisfied: httpx<1,>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from gql[httpx]>=3.0.0->weave) (0.28.1)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (25.4.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.23.0->weave) (0.27.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\nCollecting abnf~=2.2.0 (from polyfile-weave->weave)\n  Downloading abnf-2.2.0-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: chardet>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (5.2.0)\nCollecting cint>=1.0.0 (from polyfile-weave->weave)\n  Downloading cint-1.0.0-py3-none-any.whl.metadata (511 bytes)\nCollecting fickling>=0.0.8 (from polyfile-weave->weave)\n  Downloading fickling-0.1.6-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: graphviz>=0.20.1 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (0.21)\nCollecting intervaltree>=2.4.0 (from polyfile-weave->weave)\n  Downloading intervaltree-3.2.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: jinja2>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (3.1.6)\nCollecting kaitaistruct~=0.10 (from polyfile-weave->weave)\n  Downloading kaitaistruct-0.11-py2.py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: networkx>=2.6.3 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (3.5)\nCollecting pdfminer.six<=20250506,>=20220524 (from polyfile-weave->weave)\n  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave) (11.3.0)\nCollecting setuptools>=80.9.0 (from polyfile-weave->weave)\n  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\nCollecting stdlib-list~=0.11.1 (from fickling>=0.0.8->polyfile-weave->weave)\n  Downloading stdlib_list-0.11.1-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->gql[httpx]>=3.0.0->weave) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.27.0->gql[httpx]>=3.0.0->weave) (0.16.0)\nRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from intervaltree>=2.4.0->polyfile-weave->weave) (2.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.1.0->polyfile-weave->weave) (3.0.3)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (46.0.3)\nRequirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql>=3.0.0->gql[httpx]>=3.0.0->weave) (6.7.0)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql>=3.0.0->gql[httpx]>=3.0.0->weave) (0.4.1)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (2.23)\nDownloading weave-0.52.22-py3-none-any.whl (788 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gql-4.0.0-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading polyfile_weave-0.5.7-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading abnf-2.2.0-py3-none-any.whl (39 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading cint-1.0.0-py3-none-any.whl (5.6 kB)\nDownloading fickling-0.1.6-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading intervaltree-3.2.1-py2.py3-none-any.whl (25 kB)\nDownloading kaitaistruct-0.11-py2.py3-none-any.whl (11 kB)\nDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading stdlib_list-0.11.1-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: stdlib-list, setuptools, kaitaistruct, intervaltree, graphql-core, diskcache, cint, backoff, abnf, gql, fickling, pdfminer.six, polyfile-weave, weave\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.2.0\n    Uninstalling setuptools-75.2.0:\n      Successfully uninstalled setuptools-75.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.2.19 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed abnf-2.2.0 backoff-2.2.1 cint-1.0.0 diskcache-5.6.3 fickling-0.1.6 gql-4.0.0 graphql-core-3.2.7 intervaltree-3.2.1 kaitaistruct-0.11 pdfminer.six-20250506 polyfile-weave-0.5.7 setuptools-80.9.0 stdlib-list-0.11.1 weave-0.52.22\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://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@github.com/tuikhongtenbo/DS310","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:56:38.422379Z","iopub.execute_input":"2026-01-05T13:56:38.422798Z","iopub.status.idle":"2026-01-05T13:56:40.676418Z","shell.execute_reply.started":"2026-01-05T13:56:38.422764Z","shell.execute_reply":"2026-01-05T13:56:40.675700Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Cloning into 'DS310'...\nremote: Enumerating objects: 179, done.\u001b[K\nremote: Counting objects: 100% (179/179), done.\u001b[K\nremote: Compressing objects: 100% (125/125), done.\u001b[K\nremote: Total 179 (delta 94), reused 132 (delta 50), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (179/179), 22.19 MiB | 23.28 MiB/s, done.\nResolving deltas: 100% (94/94), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%cd /kaggle/working/DS310/TH3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:56:40.678569Z","iopub.execute_input":"2026-01-05T13:56:40.679193Z","iopub.status.idle":"2026-01-05T13:56:40.685189Z","shell.execute_reply.started":"2026-01-05T13:56:40.679164Z","shell.execute_reply":"2026-01-05T13:56:40.684618Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/DS310/TH3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"DS310-ViQuAD2-QA\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:56:40.686124Z","iopub.execute_input":"2026-01-05T13:56:40.686389Z","iopub.status.idle":"2026-01-05T13:56:40.801805Z","shell.execute_reply.started":"2026-01-05T13:56:40.686369Z","shell.execute_reply":"2026-01-05T13:56:40.801309Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# XLM-R finetune","metadata":{}},{"cell_type":"code","source":"MODEL_NAME=\"xlm-roberta-base\"\nOUTPUT_DIR=\"/kaggle/working/xlmr-base\"\n\n!python run_qa.py \\\n  --model_name_or_path $MODEL_NAME \\\n  --train_file \"/kaggle/input/uit-viquad-2-0/train.json\" \\\n  --validation_file \"/kaggle/input/uit-viquad-2-0/dev.json\" \\\n  --do_train \\\n  --do_eval \\\n  --eval_strategy steps \\\n  --save_strategy steps \\\n  --per_device_train_batch_size 32 \\\n  --per_device_eval_batch_size 12 \\\n  --gradient_accumulation_steps 5 \\\n  --learning_rate 1e-4 \\\n  --num_train_epochs 2 \\\n  --warmup_ratio 0.1 \\\n  --max_seq_length 314 \\\n  --doc_stride 128 \\\n  --fp16 \\\n  --dataloader_num_workers 4 \\\n  --logging_steps 100 \\\n  --save_steps 500 \\\n  --eval_steps 500 \\\n  --load_best_model_at_end \\\n  --metric_for_best_model f1 \\\n  --greater_is_better true \\\n  --save_total_limit 3 \\\n  --seed 53 \\\n  --weight_decay 0.01 \\\n  --adam_epsilon 1e-8 \\\n  --max_grad_norm 1.0 \\\n  --output_dir $OUTPUT_DIR \\\n  --version_2_with_negative \\\n  --n_best_size 20 \\\n  --max_answer_length 30 \\\n  --null_score_diff_threshold 0.0 \\\n  --overwrite_output_dir \\\n  --report_to wandb \\\n  --run_name \"xlmr-base-2epoch-bs32\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:56:40.804549Z","iopub.execute_input":"2026-01-05T13:56:40.804798Z","iopub.status.idle":"2026-01-05T14:23:42.074921Z","shell.execute_reply.started":"2026-01-05T13:56:40.804765Z","shell.execute_reply":"2026-01-05T14:23:42.074157Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2026-01-05 13:56:51.677803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767621411.836989     138 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767621411.891122     138 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767621412.281591     138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767621412.281639     138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767621412.281644     138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767621412.281647     138 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nWARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\nINFO:__main__:Training/evaluation parameters TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=True,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=4,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=500,\neval_strategy=IntervalStrategy.STEPS,\neval_use_gather_object=False,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=5,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=None,\nhub_revision=None,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=no,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0001,\nlength_column_name=length,\nliger_kernel_config=None,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=/kaggle/working/xlmr-base/runs/Jan05_13-57-08_e6c80e9e83be,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=100,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_kwargs={},\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=f1,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=2.0,\noptim=OptimizerNames.ADAMW_TORCH_FUSED,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=/kaggle/working/xlmr-base,\noverwrite_output_dir=True,\nparallelism_config=None,\npast_index=-1,\nper_device_eval_batch_size=12,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\nproject=huggingface,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=xlmr-base-2epoch-bs32,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=SaveStrategy.STEPS,\nsave_total_limit=3,\nseed=53,\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\ntrackio_space_id=trackio,\nuse_cpu=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\n)\nUsing custom data configuration default-428800dcce561d93\nINFO:datasets.builder:Using custom data configuration default-428800dcce561d93\nGenerating dataset json (/root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nINFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nINFO:datasets.builder:Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nDownloading took 0.0 min\nINFO:datasets.download.download_manager:Downloading took 0.0 min\nChecksum Computation took 0.0 min\nINFO:datasets.download.download_manager:Checksum Computation took 0.0 min\nGenerating train split\nINFO:datasets.builder:Generating train split\nGenerating train split: 22765 examples [00:02, 8842.97 examples/s]\nGenerating validation split\nINFO:datasets.builder:Generating validation split\nGenerating validation split: 5692 examples [00:00, 6050.36 examples/s]\nUnable to verify splits sizes.\nINFO:datasets.utils.info_utils:Unable to verify splits sizes.\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\nINFO:datasets.builder:Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\nconfig.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:00<00:00, 3.86MB/s]\n[INFO|configuration_utils.py:765] 2026-01-05 13:57:12,973 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 13:57:12,980 >> Model config XLMRobertaConfig {\n  \"architectures\": [\n    \"XLMRobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"xlm-roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 250002\n}\n\ntokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00<00:00, 196kB/s]\n[INFO|configuration_utils.py:765] 2026-01-05 13:57:13,146 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 13:57:13,147 >> Model config XLMRobertaConfig {\n  \"architectures\": [\n    \"XLMRobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"xlm-roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 250002\n}\n\nsentencepiece.bpe.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.07M/5.07M [00:00<00:00, 13.6MB/s]\ntokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.10M/9.10M [00:00<00:00, 20.6MB/s]\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 13:57:14,789 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/sentencepiece.bpe.model\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 13:57:14,789 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer.json\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 13:57:14,789 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 13:57:14,789 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 13:57:14,789 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2095] 2026-01-05 13:57:14,789 >> loading file chat_template.jinja from cache at None\n[INFO|configuration_utils.py:765] 2026-01-05 13:57:14,789 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 13:57:14,790 >> Model config XLMRobertaConfig {\n  \"architectures\": [\n    \"XLMRobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"xlm-roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 250002\n}\n\nmodel.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.12G/1.12G [00:03<00:00, 341MB/s]\n[INFO|modeling_utils.py:1172] 2026-01-05 13:57:19,277 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n[INFO|modeling_utils.py:5525] 2026-01-05 13:57:19,350 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:5535] 2026-01-05 13:57:19,350 >> Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nRunning tokenizer on train dataset:   0%|      | 0/22765 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-8378790e0ece72e2.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-8378790e0ece72e2.arrow\nRunning tokenizer on train dataset: 100%|â–ˆ| 22765/22765 [00:14<00:00, 1604.21 ex\nRunning tokenizer on validation dataset:   0%|  | 0/5692 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-ab0fbe4cd5472cc9.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-428800dcce561d93/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-ab0fbe4cd5472cc9.arrow\nRunning tokenizer on validation dataset: 100%|â–ˆ| 5692/5692 [00:04<00:00, 1359.48\nDownloading builder script: 6.47kB [00:00, 16.5MB/s]\nDownloading extra modules: 11.3kB [00:00, 24.7MB/s]\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n[INFO|trainer.py:749] 2026-01-05 13:57:41,937 >> Using auto half precision backend\n[INFO|trainer.py:2519] 2026-01-05 13:57:42,369 >> ***** Running training *****\n[INFO|trainer.py:2520] 2026-01-05 13:57:42,369 >>   Num examples = 26,493\n[INFO|trainer.py:2521] 2026-01-05 13:57:42,369 >>   Num Epochs = 2\n[INFO|trainer.py:2522] 2026-01-05 13:57:42,369 >>   Instantaneous batch size per device = 32\n[INFO|trainer.py:2525] 2026-01-05 13:57:42,369 >>   Total train batch size (w. parallel, distributed & accumulation) = 160\n[INFO|trainer.py:2526] 2026-01-05 13:57:42,369 >>   Gradient Accumulation steps = 5\n[INFO|trainer.py:2527] 2026-01-05 13:57:42,369 >>   Total optimization steps = 332\n[INFO|trainer.py:2528] 2026-01-05 13:57:42,370 >>   Number of trainable parameters = 277,454,594\n[INFO|integration_utils.py:867] 2026-01-05 13:57:42,370 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mply58509\u001b[0m (\u001b[33mply58509-uit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/DS310/TH3/wandb/run-20260105_135742-mk0552y3\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mxlmr-base-2epoch-bs32\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ply58509-uit/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ply58509-uit/huggingface/runs/mk0552y3\u001b[0m\n  0%|                                                   | 0/332 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{'loss': 3.3373, 'grad_norm': 12.7039155960083, 'learning_rate': 7.818791946308725e-05, 'epoch': 0.6}\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 166/332 [12:10<10:43,  3.88s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{'loss': 1.6449, 'grad_norm': 10.301857948303223, 'learning_rate': 4.463087248322148e-05, 'epoch': 1.21}\n{'loss': 1.393, 'grad_norm': 8.665206909179688, 'learning_rate': 1.1073825503355706e-05, 'epoch': 1.81}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [24:21<00:00,  3.88s/it][INFO|trainer.py:4309] 2026-01-05 14:22:05,832 >> Saving model checkpoint to /kaggle/working/xlmr-base/checkpoint-332\n[INFO|configuration_utils.py:491] 2026-01-05 14:22:05,834 >> Configuration saved in /kaggle/working/xlmr-base/checkpoint-332/config.json\n[INFO|modeling_utils.py:4181] 2026-01-05 14:22:08,118 >> Model weights saved in /kaggle/working/xlmr-base/checkpoint-332/model.safetensors\n[INFO|tokenization_utils_base.py:2590] 2026-01-05 14:22:08,120 >> tokenizer config file saved in /kaggle/working/xlmr-base/checkpoint-332/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2599] 2026-01-05 14:22:08,120 >> Special tokens file saved in /kaggle/working/xlmr-base/checkpoint-332/special_tokens_map.json\n[INFO|trainer.py:2810] 2026-01-05 14:22:11,520 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 1469.1508, 'train_samples_per_second': 36.066, 'train_steps_per_second': 0.226, 'train_loss': 2.047818769891578, 'epoch': 2.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [24:27<00:00,  4.42s/it]\n[INFO|trainer.py:4309] 2026-01-05 14:22:11,529 >> Saving model checkpoint to /kaggle/working/xlmr-base\n[INFO|configuration_utils.py:491] 2026-01-05 14:22:11,531 >> Configuration saved in /kaggle/working/xlmr-base/config.json\n[INFO|modeling_utils.py:4181] 2026-01-05 14:22:13,924 >> Model weights saved in /kaggle/working/xlmr-base/model.safetensors\n[INFO|tokenization_utils_base.py:2590] 2026-01-05 14:22:13,926 >> tokenizer config file saved in /kaggle/working/xlmr-base/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2599] 2026-01-05 14:22:13,927 >> Special tokens file saved in /kaggle/working/xlmr-base/special_tokens_map.json\n***** train metrics *****\n  epoch                    =        2.0\n  total_flos               =  7907787GF\n  train_loss               =     2.0478\n  train_runtime            = 0:24:29.15\n  train_samples            =      26493\n  train_samples_per_second =     36.066\n  train_steps_per_second   =      0.226\nINFO:__main__:*** Evaluate ***\n[INFO|trainer.py:1012] 2026-01-05 14:22:13,981 >> The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `XLMRobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4643] 2026-01-05 14:22:13,988 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4645] 2026-01-05 14:22:13,989 >>   Num examples = 6594\n[INFO|trainer.py:4648] 2026-01-05 14:22:13,989 >>   Batch size = 12\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [01:00<00:00,  8.61it/s]INFO:utils_qa:Post-processing 5692 example predictions split into 6594 features.\n\n  0%|                                                  | 0/5692 [00:00<?, ?it/s]\u001b[A\n  1%|â–                                       | 40/5692 [00:00<00:14, 391.23it/s]\u001b[A\n  1%|â–Œ                                       | 80/5692 [00:00<00:15, 371.37it/s]\u001b[A\n  2%|â–Š                                      | 118/5692 [00:00<00:14, 372.95it/s]\u001b[A\n  3%|â–ˆ                                      | 157/5692 [00:00<00:14, 377.42it/s]\u001b[A\n  3%|â–ˆâ–                                     | 195/5692 [00:00<00:14, 369.77it/s]\u001b[A\n  4%|â–ˆâ–Œ                                     | 234/5692 [00:00<00:14, 376.21it/s]\u001b[A\n  5%|â–ˆâ–Š                                     | 272/5692 [00:00<00:14, 368.40it/s]\u001b[A\n  5%|â–ˆâ–ˆ                                     | 309/5692 [00:00<00:14, 366.59it/s]\u001b[A\n  6%|â–ˆâ–ˆâ–                                    | 346/5692 [00:00<00:14, 356.93it/s]\u001b[A\n  7%|â–ˆâ–ˆâ–Œ                                    | 383/5692 [00:01<00:14, 360.36it/s]\u001b[A\n  7%|â–ˆâ–ˆâ–‰                                    | 422/5692 [00:01<00:14, 367.57it/s]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆâ–                                   | 462/5692 [00:01<00:13, 376.65it/s]\u001b[A\n  9%|â–ˆâ–ˆâ–ˆâ–                                   | 500/5692 [00:01<00:14, 356.93it/s]\u001b[A\n  9%|â–ˆâ–ˆâ–ˆâ–‹                                   | 539/5692 [00:01<00:14, 362.72it/s]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–‰                                   | 576/5692 [00:01<00:14, 357.94it/s]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 614/5692 [00:01<00:13, 363.07it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 655/5692 [00:01<00:13, 372.52it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 694/5692 [00:01<00:13, 376.54it/s]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 733/5692 [00:01<00:13, 379.47it/s]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 771/5692 [00:02<00:13, 367.71it/s]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 808/5692 [00:02<00:13, 363.81it/s]\u001b[A\n 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 847/5692 [00:02<00:13, 369.45it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 885/5692 [00:02<00:13, 367.18it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 922/5692 [00:02<00:12, 367.15it/s]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 960/5692 [00:02<00:12, 369.45it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 1003/5692 [00:02<00:12, 385.24it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 1042/5692 [00:02<00:12, 386.51it/s]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1082/5692 [00:02<00:11, 387.89it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1122/5692 [00:03<00:11, 388.74it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 1161/5692 [00:03<00:11, 381.60it/s]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 1200/5692 [00:03<00:11, 381.24it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 1239/5692 [00:03<00:11, 374.30it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 1277/5692 [00:03<00:12, 367.90it/s]\u001b[A\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 1315/5692 [00:03<00:11, 366.90it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 1352/5692 [00:03<00:12, 349.74it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1388/5692 [00:03<00:12, 334.46it/s]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1422/5692 [00:03<00:13, 327.18it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 1461/5692 [00:03<00:12, 342.81it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 1496/5692 [00:04<00:12, 344.49it/s]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1533/5692 [00:04<00:11, 351.24it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1569/5692 [00:04<00:12, 341.00it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 1609/5692 [00:04<00:11, 357.17it/s]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 1645/5692 [00:04<00:11, 356.45it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 1682/5692 [00:04<00:11, 359.14it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 1720/5692 [00:04<00:10, 363.58it/s]\u001b[A\n 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 1759/5692 [00:04<00:10, 369.59it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 1797/5692 [00:04<00:10, 358.00it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 1834/5692 [00:05<00:10, 358.29it/s]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 1870/5692 [00:05<00:10, 355.92it/s]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 1906/5692 [00:05<00:10, 351.70it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 1944/5692 [00:05<00:10, 358.64it/s]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 1980/5692 [00:05<00:10, 353.99it/s]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2016/5692 [00:05<00:10, 345.72it/s]\u001b[A\n 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 2053/5692 [00:05<00:10, 352.23it/s]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 2090/5692 [00:05<00:10, 340.40it/s]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2125/5692 [00:05<00:10, 340.14it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2160/5692 [00:05<00:10, 326.38it/s]\u001b[A\n 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 2195/5692 [00:06<00:10, 331.73it/s]\u001b[A\n 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 2233/5692 [00:06<00:10, 343.29it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2270/5692 [00:06<00:09, 348.33it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2305/5692 [00:06<00:09, 345.19it/s]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 2343/5692 [00:06<00:09, 348.77it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 2378/5692 [00:06<00:09, 342.72it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 2413/5692 [00:06<00:09, 343.02it/s]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2452/5692 [00:06<00:09, 354.55it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 2488/5692 [00:06<00:09, 352.18it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 2524/5692 [00:07<00:09, 350.89it/s]\u001b[A\n 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 2565/5692 [00:07<00:08, 367.39it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2605/5692 [00:07<00:08, 374.90it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 2643/5692 [00:07<00:08, 371.82it/s]\u001b[A\n 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 2681/5692 [00:07<00:08, 371.48it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 2719/5692 [00:07<00:08, 369.78it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 2757/5692 [00:07<00:07, 370.32it/s]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 2798/5692 [00:07<00:07, 380.11it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 2837/5692 [00:07<00:07, 361.91it/s]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 2876/5692 [00:07<00:07, 369.86it/s]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 2914/5692 [00:08<00:07, 371.99it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 2952/5692 [00:08<00:07, 356.74it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 2988/5692 [00:08<00:07, 351.43it/s]\u001b[A\n 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 3024/5692 [00:08<00:07, 352.74it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 3064/5692 [00:08<00:07, 365.77it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 3101/5692 [00:08<00:07, 362.27it/s]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 3140/5692 [00:08<00:06, 370.15it/s]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 3178/5692 [00:08<00:06, 371.01it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 3216/5692 [00:08<00:06, 364.38it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 3253/5692 [00:09<00:06, 361.16it/s]\u001b[A\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 3291/5692 [00:09<00:06, 364.72it/s]\u001b[A\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 3328/5692 [00:09<00:06, 362.55it/s]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 3365/5692 [00:09<00:06, 356.79it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 3401/5692 [00:09<00:06, 343.78it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 3437/5692 [00:09<00:06, 347.61it/s]\u001b[A\n 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 3472/5692 [00:09<00:06, 343.83it/s]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 3508/5692 [00:09<00:06, 346.90it/s]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 3546/5692 [00:09<00:06, 354.76it/s]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 3585/5692 [00:09<00:05, 364.55it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3622/5692 [00:10<00:05, 362.48it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3659/5692 [00:10<00:05, 357.66it/s]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 3696/5692 [00:10<00:05, 361.02it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 3735/5692 [00:10<00:05, 367.10it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3774/5692 [00:10<00:05, 372.99it/s]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3812/5692 [00:10<00:05, 364.70it/s]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 3849/5692 [00:10<00:05, 362.96it/s]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 3891/5692 [00:10<00:04, 377.03it/s]\u001b[A\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 3931/5692 [00:10<00:04, 381.12it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 3970/5692 [00:10<00:04, 380.16it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 4009/5692 [00:11<00:04, 379.63it/s]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 4047/5692 [00:11<00:04, 376.09it/s]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 4088/5692 [00:11<00:04, 385.26it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 4127/5692 [00:11<00:04, 379.16it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 4166/5692 [00:11<00:04, 380.83it/s]\u001b[A\n 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 4205/5692 [00:11<00:04, 364.77it/s]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 4242/5692 [00:11<00:03, 365.23it/s]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 4282/5692 [00:11<00:03, 373.11it/s]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 4322/5692 [00:11<00:03, 373.78it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 4360/5692 [00:12<00:03, 375.36it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 4398/5692 [00:12<00:03, 356.57it/s]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 4434/5692 [00:12<00:03, 353.91it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 4474/5692 [00:12<00:03, 365.78it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 4514/5692 [00:12<00:03, 374.94it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 4552/5692 [00:12<00:03, 364.34it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 4591/5692 [00:12<00:03, 366.59it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 4632/5692 [00:12<00:02, 376.60it/s]\u001b[A\n 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 4670/5692 [00:12<00:02, 359.79it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 4707/5692 [00:12<00:02, 361.73it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 4744/5692 [00:13<00:02, 355.39it/s]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 4782/5692 [00:13<00:02, 360.04it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 4819/5692 [00:13<00:02, 362.40it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 4856/5692 [00:13<00:02, 356.28it/s]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4893/5692 [00:13<00:02, 359.54it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4930/5692 [00:13<00:02, 358.69it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4966/5692 [00:13<00:02, 343.25it/s]\u001b[A\n 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5001/5692 [00:13<00:02, 336.10it/s]\u001b[A\n 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5035/5692 [00:13<00:01, 336.36it/s]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5069/5692 [00:14<00:01, 333.46it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 5104/5692 [00:14<00:01, 337.88it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5138/5692 [00:14<00:01, 315.90it/s]\u001b[A\n 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 5170/5692 [00:14<00:01, 313.08it/s]\u001b[A\n 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 5208/5692 [00:14<00:01, 330.10it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 5246/5692 [00:14<00:01, 343.13it/s]\u001b[A\n 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5282/5692 [00:14<00:01, 347.18it/s]\u001b[A\n 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5317/5692 [00:14<00:01, 344.96it/s]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 5352/5692 [00:14<00:00, 345.93it/s]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 5390/5692 [00:14<00:00, 355.92it/s]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5426/5692 [00:15<00:00, 352.26it/s]\u001b[A\n 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5464/5692 [00:15<00:00, 359.22it/s]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5506/5692 [00:15<00:00, 375.20it/s]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5546/5692 [00:15<00:00, 380.95it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5586/5692 [00:15<00:00, 385.25it/s]\u001b[A\n 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5625/5692 [00:15<00:00, 375.70it/s]\u001b[A\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5692/5692 [00:15<00:00, 360.49it/s]\u001b[A\nINFO:utils_qa:Saving predictions to /kaggle/working/xlmr-base/eval_predictions.json.\nINFO:utils_qa:Saving nbest_preds to /kaggle/working/xlmr-base/eval_nbest_predictions.json.\nINFO:utils_qa:Saving null_odds to /kaggle/working/xlmr-base/eval_null_odds.json.\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [01:23<00:00,  6.59it/s]\n***** eval metrics *****\n  epoch                   =        2.0\n  eval_HasAns_exact       =    38.4995\n  eval_HasAns_f1          =      57.93\n  eval_HasAns_total       =       3852\n  eval_NoAns_exact        =    37.8804\n  eval_NoAns_f1           =    37.8804\n  eval_NoAns_total        =       1840\n  eval_best_exact         =    38.3169\n  eval_best_exact_thresh  =        0.0\n  eval_best_f1            =    51.4663\n  eval_best_f1_thresh     =        0.0\n  eval_exact              =    38.2994\n  eval_f1                 =    51.4488\n  eval_runtime            = 0:01:00.61\n  eval_samples            =       6594\n  eval_samples_per_second =    108.778\n  eval_steps_per_second   =      9.073\n  eval_total              =       5692\n[INFO|modelcard.py:456] 2026-01-05 14:23:38,063 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: ğŸš€ View run \u001b[33mxlmr-base-2epoch-bs32\u001b[0m at: \u001b[34m\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20260105_135742-mk0552y3/logs\u001b[0m\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"OUTPUT_DIR=\"/kaggle/working/xlmr-base\"\n\n!python run_qa.py \\\n  --model_name_or_path $OUTPUT_DIR \\\n  --test_file \"/kaggle/input/uit-viquad-2-0/Private_Test_ref.json\" \\\n  --do_predict \\\n  --per_device_eval_batch_size 12 \\\n  --max_seq_length 512 \\\n  --doc_stride 128 \\\n  --dataloader_num_workers 4 \\\n  --output_dir $OUTPUT_DIR \\\n  --version_2_with_negative \\\n  --n_best_size 20 \\\n  --max_answer_length 30 \\\n  --null_score_diff_threshold 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:30:23.019503Z","iopub.execute_input":"2026-01-05T14:30:23.019842Z","iopub.status.idle":"2026-01-05T14:33:18.826175Z","shell.execute_reply.started":"2026-01-05T14:30:23.019809Z","shell.execute_reply":"2026-01-05T14:33:18.825479Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2026-01-05 14:30:27.765183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767623427.786815     284 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767623427.793463     284 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767623427.811483     284 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767623427.811521     284 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767623427.811524     284 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767623427.811528     284 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nWARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\nINFO:__main__:Training/evaluation parameters TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=True,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=4,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=False,\ndo_predict=True,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=None,\neval_strategy=IntervalStrategy.NO,\neval_use_gather_object=False,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=None,\nhub_revision=None,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=no,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nliger_kernel_config=None,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=/kaggle/working/xlmr-base/runs/Jan05_14-30-33_e6c80e9e83be,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_kwargs={},\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=OptimizerNames.ADAMW_TORCH_FUSED,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=/kaggle/working/xlmr-base,\noverwrite_output_dir=False,\nparallelism_config=None,\npast_index=-1,\nper_device_eval_batch_size=12,\nper_device_train_batch_size=8,\nprediction_loss_only=False,\nproject=huggingface,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=None,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=SaveStrategy.STEPS,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\ntrackio_space_id=trackio,\nuse_cpu=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.0,\n)\nUsing custom data configuration default-aecb69af4c19754c\nINFO:datasets.builder:Using custom data configuration default-aecb69af4c19754c\nGenerating dataset json (/root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nINFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91)\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nINFO:datasets.builder:Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91...\nDownloading took 0.0 min\nINFO:datasets.download.download_manager:Downloading took 0.0 min\nChecksum Computation took 0.0 min\nINFO:datasets.download.download_manager:Checksum Computation took 0.0 min\nGenerating test split\nINFO:datasets.builder:Generating test split\nGenerating test split: 7301 examples [00:00, 16133.28 examples/s]\nUnable to verify splits sizes.\nINFO:datasets.utils.info_utils:Unable to verify splits sizes.\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\nINFO:datasets.builder:Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91. Subsequent calls will reuse this data.\n[INFO|configuration_utils.py:763] 2026-01-05 14:30:35,783 >> loading configuration file /kaggle/working/xlmr-base/config.json\n[INFO|configuration_utils.py:839] 2026-01-05 14:30:35,786 >> Model config XLMRobertaConfig {\n  \"architectures\": [\n    \"XLMRobertaForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"dtype\": \"float32\",\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"xlm-roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.57.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 250002\n}\n\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 14:30:35,796 >> loading file sentencepiece.bpe.model\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 14:30:35,796 >> loading file tokenizer.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 14:30:35,796 >> loading file added_tokens.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 14:30:35,796 >> loading file special_tokens_map.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 14:30:35,796 >> loading file tokenizer_config.json\n[INFO|tokenization_utils_base.py:2093] 2026-01-05 14:30:35,796 >> loading file chat_template.jinja\n[INFO|modeling_utils.py:1169] 2026-01-05 14:30:36,490 >> loading weights file /kaggle/working/xlmr-base/model.safetensors\nRunning tokenizer on prediction dataset:   0%|  | 0/7301 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-4d14f8d7b2fd4fa2.arrow\nINFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-aecb69af4c19754c/0.0.0/4d80ec5faec344ea842e25017a662668bf00c8eec2b59a13536060e6e7492b91/cache-4d14f8d7b2fd4fa2.arrow\nRunning tokenizer on prediction dataset: 100%|â–ˆ| 7301/7301 [00:05<00:00, 1222.47\nINFO:__main__:*** Predict ***\n[INFO|trainer.py:1012] 2026-01-05 14:30:43,912 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `XLMRobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n[INFO|trainer.py:4643] 2026-01-05 14:30:43,921 >> \n***** Running Prediction *****\n[INFO|trainer.py:4645] 2026-01-05 14:30:43,921 >>   Num examples = 7374\n[INFO|trainer.py:4648] 2026-01-05 14:30:43,921 >>   Batch size = 12\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [01:57<00:00,  5.57it/s]INFO:utils_qa:Post-processing 7301 example predictions split into 7374 features.\n\n  0%|                                                  | 0/7301 [00:00<?, ?it/s]\u001b[A\n  0%|â–                                       | 33/7301 [00:00<00:22, 325.23it/s]\u001b[A\n  1%|â–                                       | 67/7301 [00:00<00:21, 330.44it/s]\u001b[A\n  1%|â–Œ                                      | 102/7301 [00:00<00:21, 338.85it/s]\u001b[A\n  2%|â–‹                                      | 138/7301 [00:00<00:20, 343.26it/s]\u001b[A\n  2%|â–‰                                      | 173/7301 [00:00<00:21, 326.57it/s]\u001b[A\n  3%|â–ˆ                                      | 206/7301 [00:00<00:27, 261.45it/s]\u001b[A\n  3%|â–ˆâ–                                     | 234/7301 [00:00<00:27, 258.84it/s]\u001b[A\n  4%|â–ˆâ–                                     | 262/7301 [00:00<00:30, 229.41it/s]\u001b[A\n  4%|â–ˆâ–Œ                                     | 295/7301 [00:01<00:27, 254.75it/s]\u001b[A\n  4%|â–ˆâ–‹                                     | 327/7301 [00:01<00:25, 271.38it/s]\u001b[A\n  5%|â–ˆâ–‰                                     | 361/7301 [00:01<00:24, 289.08it/s]\u001b[A\n  5%|â–ˆâ–ˆ                                     | 394/7301 [00:01<00:23, 299.62it/s]\u001b[A\n  6%|â–ˆâ–ˆâ–                                    | 428/7301 [00:01<00:22, 309.83it/s]\u001b[A\n  6%|â–ˆâ–ˆâ–                                    | 463/7301 [00:01<00:21, 320.03it/s]\u001b[A\n  7%|â–ˆâ–ˆâ–‹                                    | 498/7301 [00:01<00:20, 328.55it/s]\u001b[A\n  7%|â–ˆâ–ˆâ–Š                                    | 534/7301 [00:01<00:20, 334.86it/s]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆ                                    | 568/7301 [00:01<00:20, 333.89it/s]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆâ–                                   | 602/7301 [00:01<00:20, 332.70it/s]\u001b[A\n  9%|â–ˆâ–ˆâ–ˆâ–                                   | 636/7301 [00:02<00:19, 334.12it/s]\u001b[A\n  9%|â–ˆâ–ˆâ–ˆâ–Œ                                   | 670/7301 [00:02<00:20, 327.20it/s]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–Š                                   | 704/7301 [00:02<00:20, 328.37it/s]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–‰                                   | 738/7301 [00:02<00:19, 330.84it/s]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆ                                   | 772/7301 [00:02<00:19, 331.00it/s]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 806/7301 [00:02<00:19, 332.09it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 840/7301 [00:02<00:20, 323.02it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 874/7301 [00:02<00:19, 327.74it/s]\u001b[A\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 908/7301 [00:02<00:19, 329.03it/s]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 941/7301 [00:03<00:19, 327.97it/s]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 974/7301 [00:03<00:19, 327.01it/s]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 1007/7301 [00:03<00:19, 323.68it/s]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 1040/7301 [00:03<00:20, 305.83it/s]\u001b[A\n 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 1071/7301 [00:03<00:20, 304.44it/s]\u001b[A\n 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 1103/7301 [00:03<00:20, 307.84it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 1135/7301 [00:03<00:19, 311.13it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 1168/7301 [00:03<00:19, 314.20it/s]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 1201/7301 [00:03<00:19, 316.68it/s]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 1234/7301 [00:03<00:18, 319.97it/s]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 1268/7301 [00:04<00:18, 325.18it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 1302/7301 [00:04<00:18, 328.64it/s]\u001b[A\n 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 1335/7301 [00:04<00:18, 323.94it/s]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 1368/7301 [00:04<00:18, 325.59it/s]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1404/7301 [00:04<00:17, 333.88it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1439/7301 [00:04<00:17, 337.50it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 1473/7301 [00:04<00:17, 327.56it/s]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 1506/7301 [00:04<00:17, 327.23it/s]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 1540/7301 [00:04<00:17, 328.02it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 1574/7301 [00:04<00:17, 331.20it/s]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 1609/7301 [00:05<00:16, 335.07it/s]\u001b[A\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 1643/7301 [00:05<00:16, 336.28it/s]\u001b[A\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 1677/7301 [00:05<00:16, 336.19it/s]\u001b[A\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 1711/7301 [00:05<00:16, 334.65it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 1745/7301 [00:05<00:16, 332.98it/s]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1779/7301 [00:05<00:16, 332.36it/s]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 1813/7301 [00:05<00:16, 333.11it/s]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 1847/7301 [00:05<00:16, 328.01it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 1881/7301 [00:05<00:16, 329.35it/s]\u001b[A\n 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 1914/7301 [00:06<00:16, 329.31it/s]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1949/7301 [00:06<00:16, 332.83it/s]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1983/7301 [00:06<00:16, 325.43it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 2016/7301 [00:06<00:16, 324.18it/s]\u001b[A\n 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 2049/7301 [00:06<00:16, 324.11it/s]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 2082/7301 [00:06<00:16, 324.53it/s]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 2115/7301 [00:06<00:15, 325.61it/s]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 2148/7301 [00:06<00:15, 325.49it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 2181/7301 [00:06<00:15, 325.85it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2215/7301 [00:06<00:15, 327.96it/s]\u001b[A\n 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 2248/7301 [00:07<00:15, 327.49it/s]\u001b[A\n 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 2281/7301 [00:07<00:15, 327.33it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 2314/7301 [00:07<00:15, 323.12it/s]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2347/7301 [00:07<00:15, 321.70it/s]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2381/7301 [00:07<00:15, 324.98it/s]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 2415/7301 [00:07<00:14, 326.69it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 2448/7301 [00:07<00:14, 326.61it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 2481/7301 [00:07<00:14, 325.42it/s]\u001b[A\n 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 2514/7301 [00:07<00:14, 324.63it/s]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2548/7301 [00:07<00:14, 326.28it/s]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2581/7301 [00:08<00:14, 325.82it/s]\u001b[A\n 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 2614/7301 [00:08<00:15, 303.99it/s]\u001b[A\n 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 2648/7301 [00:08<00:14, 311.49it/s]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 2680/7301 [00:08<00:14, 309.04it/s]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2712/7301 [00:08<00:14, 310.79it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2745/7301 [00:08<00:14, 315.12it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 2777/7301 [00:08<00:14, 313.82it/s]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 2809/7301 [00:08<00:14, 312.85it/s]\u001b[A\n 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2841/7301 [00:08<00:14, 309.43it/s]\u001b[A\n 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 2872/7301 [00:09<00:14, 303.87it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 2903/7301 [00:09<00:15, 292.89it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2933/7301 [00:09<00:15, 289.11it/s]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2962/7301 [00:09<00:15, 288.18it/s]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2991/7301 [00:09<00:15, 286.19it/s]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 3022/7301 [00:09<00:14, 291.65it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 3052/7301 [00:09<00:14, 289.90it/s]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 3086/7301 [00:09<00:13, 303.40it/s]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 3119/7301 [00:09<00:13, 311.14it/s]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 3152/7301 [00:09<00:13, 315.73it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 3184/7301 [00:10<00:13, 314.00it/s]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 3216/7301 [00:10<00:12, 315.52it/s]\u001b[A\n 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 3250/7301 [00:10<00:12, 321.38it/s]\u001b[A\n 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3283/7301 [00:10<00:12, 321.83it/s]\u001b[A\n 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 3316/7301 [00:10<00:12, 320.28it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 3349/7301 [00:10<00:12, 304.73it/s]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 3380/7301 [00:10<00:12, 304.05it/s]\u001b[A\n 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 3411/7301 [00:10<00:12, 305.20it/s]\u001b[A\n 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 3442/7301 [00:10<00:12, 300.47it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 3473/7301 [00:10<00:12, 300.76it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 3506/7301 [00:11<00:12, 308.50it/s]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 3537/7301 [00:11<00:12, 305.51it/s]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 3569/7301 [00:11<00:12, 308.27it/s]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 3601/7301 [00:11<00:11, 311.34it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 3633/7301 [00:11<00:11, 312.43it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 3665/7301 [00:11<00:11, 310.28it/s]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 3697/7301 [00:11<00:11, 313.10it/s]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 3730/7301 [00:11<00:11, 315.62it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 3762/7301 [00:11<00:11, 313.77it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 3794/7301 [00:12<00:11, 313.87it/s]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 3828/7301 [00:12<00:10, 319.31it/s]\u001b[A\n 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 3860/7301 [00:12<00:12, 271.73it/s]\u001b[A\n 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 3891/7301 [00:12<00:12, 280.79it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 3924/7301 [00:12<00:11, 292.93it/s]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 3956/7301 [00:12<00:11, 299.69it/s]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 3988/7301 [00:12<00:10, 305.23it/s]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 4019/7301 [00:12<00:10, 304.88it/s]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 4053/7301 [00:12<00:10, 314.50it/s]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 4085/7301 [00:12<00:10, 315.79it/s]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 4117/7301 [00:13<00:10, 315.23it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 4149/7301 [00:13<00:10, 313.30it/s]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 4181/7301 [00:13<00:10, 311.55it/s]\u001b[A\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 4213/7301 [00:13<00:10, 294.37it/s]\u001b[A\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 4246/7301 [00:13<00:10, 303.79it/s]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 4280/7301 [00:13<00:09, 312.78it/s]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 4313/7301 [00:13<00:09, 317.34it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 4345/7301 [00:13<00:09, 317.82it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 4377/7301 [00:13<00:09, 293.46it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 4411/7301 [00:14<00:09, 305.18it/s]\u001b[A\n 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 4445/7301 [00:14<00:09, 313.09it/s]\u001b[A\n 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 4478/7301 [00:14<00:08, 317.00it/s]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 4512/7301 [00:14<00:08, 323.08it/s]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 4545/7301 [00:14<00:08, 324.73it/s]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 4579/7301 [00:14<00:08, 328.90it/s]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 4613/7301 [00:14<00:08, 331.11it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 4647/7301 [00:14<00:08, 331.01it/s]\u001b[A\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 4681/7301 [00:14<00:07, 333.23it/s]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 4715/7301 [00:14<00:07, 332.43it/s]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 4750/7301 [00:15<00:07, 336.76it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 4785/7301 [00:15<00:07, 339.94it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 4820/7301 [00:15<00:07, 339.46it/s]\u001b[A\n 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 4854/7301 [00:15<00:07, 338.50it/s]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 4888/7301 [00:15<00:07, 333.14it/s]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 4922/7301 [00:15<00:07, 332.95it/s]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 4956/7301 [00:15<00:07, 331.08it/s]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 4990/7301 [00:15<00:07, 325.38it/s]\u001b[A\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 5023/7301 [00:15<00:06, 326.03it/s]\u001b[A\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 5056/7301 [00:15<00:06, 325.69it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 5089/7301 [00:16<00:06, 324.65it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 5122/7301 [00:16<00:06, 326.00it/s]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 5156/7301 [00:16<00:06, 328.85it/s]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 5189/7301 [00:16<00:06, 326.94it/s]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 5222/7301 [00:16<00:06, 326.00it/s]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 5255/7301 [00:16<00:06, 324.70it/s]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 5290/7301 [00:16<00:06, 332.12it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 5324/7301 [00:16<00:05, 333.47it/s]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 5358/7301 [00:16<00:05, 335.14it/s]\u001b[A\n 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 5392/7301 [00:16<00:05, 336.49it/s]\u001b[A\n 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 5427/7301 [00:17<00:05, 338.12it/s]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 5461/7301 [00:17<00:05, 333.55it/s]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 5495/7301 [00:17<00:05, 326.61it/s]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 5528/7301 [00:17<00:05, 326.62it/s]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 5563/7301 [00:17<00:05, 330.83it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 5598/7301 [00:17<00:05, 334.39it/s]\u001b[A\n 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 5632/7301 [00:17<00:05, 325.11it/s]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 5665/7301 [00:17<00:05, 325.55it/s]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 5698/7301 [00:17<00:05, 316.63it/s]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 5731/7301 [00:18<00:04, 319.24it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 5764/7301 [00:18<00:04, 320.09it/s]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 5798/7301 [00:18<00:04, 324.09it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 5832/7301 [00:18<00:04, 328.63it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 5865/7301 [00:18<00:04, 329.02it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 5901/7301 [00:18<00:04, 336.64it/s]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 5936/7301 [00:18<00:04, 338.45it/s]\u001b[A\n 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 5970/7301 [00:18<00:03, 334.12it/s]\u001b[A\n 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 6004/7301 [00:18<00:03, 331.91it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 6039/7301 [00:18<00:03, 335.22it/s]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 6074/7301 [00:19<00:03, 337.09it/s]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 6108/7301 [00:19<00:03, 337.21it/s]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 6142/7301 [00:19<00:03, 334.47it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 6176/7301 [00:19<00:03, 331.41it/s]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 6210/7301 [00:19<00:03, 331.80it/s]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 6244/7301 [00:19<00:03, 329.89it/s]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 6277/7301 [00:19<00:03, 328.82it/s]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 6310/7301 [00:19<00:03, 324.96it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6343/7301 [00:19<00:02, 324.03it/s]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6376/7301 [00:19<00:02, 322.54it/s]\u001b[A\n 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6409/7301 [00:20<00:02, 320.97it/s]\u001b[A\n 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 6444/7301 [00:20<00:02, 327.98it/s]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 6477/7301 [00:20<00:02, 325.50it/s]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 6512/7301 [00:20<00:02, 329.94it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6546/7301 [00:20<00:02, 329.75it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6582/7301 [00:20<00:02, 336.41it/s]\u001b[A\n 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6616/7301 [00:20<00:02, 337.08it/s]\u001b[A\n 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6650/7301 [00:20<00:01, 334.99it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6684/7301 [00:20<00:01, 336.05it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6718/7301 [00:21<00:01, 334.21it/s]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6753/7301 [00:21<00:01, 336.34it/s]\u001b[A\n 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6787/7301 [00:21<00:01, 334.08it/s]\u001b[A\n 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6821/7301 [00:21<00:01, 332.35it/s]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 6855/7301 [00:21<00:01, 332.41it/s]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 6889/7301 [00:21<00:01, 330.12it/s]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 6924/7301 [00:21<00:01, 335.47it/s]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6958/7301 [00:21<00:01, 331.93it/s]\u001b[A\n 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6992/7301 [00:21<00:00, 333.75it/s]\u001b[A\n 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 7026/7301 [00:21<00:00, 330.21it/s]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 7060/7301 [00:22<00:00, 328.50it/s]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 7093/7301 [00:22<00:00, 323.36it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 7126/7301 [00:22<00:00, 323.37it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7159/7301 [00:22<00:00, 317.75it/s]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7191/7301 [00:22<00:00, 313.67it/s]\u001b[A\n 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7225/7301 [00:22<00:00, 319.12it/s]\u001b[A\n 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 7257/7301 [00:22<00:00, 315.37it/s]\u001b[A\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7301/7301 [00:22<00:00, 319.95it/s]\u001b[A\nINFO:utils_qa:Saving predictions to /kaggle/working/xlmr-base/predict_predictions.json.\nINFO:utils_qa:Saving nbest_preds to /kaggle/working/xlmr-base/predict_nbest_predictions.json.\nINFO:utils_qa:Saving null_odds to /kaggle/working/xlmr-base/predict_null_odds.json.\n***** predict metrics *****\n  predict_samples             =       7374\n  test_NoAns_exact            =    31.7354\n  test_NoAns_f1               =    31.7354\n  test_NoAns_total            =       7301\n  test_best_exact             =      100.0\n  test_best_exact_thresh      =        0.0\n  test_best_f1                =      100.0\n  test_best_f1_thresh         =        0.0\n  test_exact                  =    31.7354\n  test_f1                     =    31.7354\n  test_model_preparation_time =     0.0028\n  test_runtime                = 0:01:58.16\n  test_samples_per_second     =     62.402\n  test_steps_per_second       =      5.204\n  test_total                  =       7301\n[INFO|modelcard.py:456] 2026-01-05 14:33:16,059 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [02:31<00:00,  4.06it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"OUTPUT_DIR=\"/kaggle/working/xlmr-base\"\n\n!python evaluation.py \\\n  \"/kaggle/input/uit-viquad-2-0/ground_truth_private_test.json\" \\\n  \"$OUTPUT_DIR/predict_predictions.json\" \\\n  --na-prob-file \"$OUTPUT_DIR/predict_null_odds.json\" \\\n  --out-file \"$OUTPUT_DIR/evaluation_results.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:33:18.827893Z","iopub.execute_input":"2026-01-05T14:33:18.828171Z","iopub.status.idle":"2026-01-05T14:33:19.749828Z","shell.execute_reply.started":"2026-01-05T14:33:18.828143Z","shell.execute_reply":"2026-01-05T14:33:19.748977Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2026-01-05 14:33:18 - INFO - Logger initialized. Log file: ./logs/evaluation_20260105_143318.log\n2026-01-05 14:33:18 - INFO - Loading dataset from /kaggle/input/uit-viquad-2-0/ground_truth_private_test.json\n2026-01-05 14:33:19 - INFO - Loading predictions from /kaggle/working/xlmr-base/predict_predictions.json\n2026-01-05 14:33:19 - INFO - Loading no-answer probabilities from /kaggle/working/xlmr-base/predict_null_odds.json\n2026-01-05 14:33:19 - INFO - Dataset contains 19 examples\n2026-01-05 14:33:19 - INFO - Predictions contain 7301 entries\n2026-01-05 14:33:19 - INFO - Evaluating on 3712 questions with predictions\n2026-01-05 14:33:19 - INFO -   Has answer: 2596\n2026-01-05 14:33:19 - INFO -   No answer: 1116\n2026-01-05 14:33:19 - INFO - Evaluation Results:\n2026-01-05 14:33:19 - INFO -   Exact Match: 43.53%\n2026-01-05 14:33:19 - INFO -   F1 Score: 52.47%\n2026-01-05 14:33:19 - INFO -   HasAns - Exact: 41.18%\n2026-01-05 14:33:19 - INFO -   HasAns - F1: 53.96%\n2026-01-05 14:33:19 - INFO -   NoAns - Exact: 49.01%\n2026-01-05 14:33:19 - INFO -   NoAns - F1: 49.01%\n2026-01-05 14:33:19 - INFO - Saving evaluation results to /kaggle/working/xlmr-base/evaluation_results.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}