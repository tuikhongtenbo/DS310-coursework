{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13901578,"sourceType":"datasetVersion","datasetId":8856847},{"sourceId":13901942,"sourceType":"datasetVersion","datasetId":8857064}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Bài 2: Thực hiện xây dựng mô hình dự đoán cho bài toán phân loại chủ đề (topic-based) dựa trên bộ dữ liệu UIT-VSFC. Lưu ý: pre-trained embedding ở 2 bài phải trùng nhau.","metadata":{}},{"cell_type":"code","source":"!pip install pyvi","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:50:54.351172Z","iopub.execute_input":"2025-11-28T01:50:54.351638Z","iopub.status.idle":"2025-11-28T01:50:59.550345Z","shell.execute_reply.started":"2025-11-28T01:50:54.351616Z","shell.execute_reply":"2025-11-28T01:50:59.549446Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pyvi\n  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.2.2)\nCollecting sklearn-crfsuite (from pyvi)\n  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\nCollecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\nRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn->pyvi) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\nDownloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\nDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\nSuccessfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom tqdm import tqdm \n\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom typing import List, Dict, Tuple\nfrom collections import Counter\nfrom pyvi import ViTokenizer\n\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:50:59.551852Z","iopub.execute_input":"2025-11-28T01:50:59.552166Z","iopub.status.idle":"2025-11-28T01:51:03.530800Z","shell.execute_reply.started":"2025-11-28T01:50:59.552132Z","shell.execute_reply":"2025-11-28T01:51:03.530206Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def read_data_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n        \n    return data\n\n# train\ntrain_data = read_data_json('/kaggle/input/uit-vsfc/UIT-VSFC-train.json')\n# dev\ndev_data = read_data_json('/kaggle/input/uit-vsfc/UIT-VSFC-dev.json')\n# test\ntest_data = read_data_json('/kaggle/input/uit-vsfc/UIT-VSFC-test.json')","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:51:03.531691Z","iopub.execute_input":"2025-11-28T01:51:03.532089Z","iopub.status.idle":"2025-11-28T01:51:03.596402Z","shell.execute_reply.started":"2025-11-28T01:51:03.532069Z","shell.execute_reply":"2025-11-28T01:51:03.595856Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{}},{"cell_type":"markdown","source":"### Build vocab","metadata":{}},{"cell_type":"code","source":"# Build vocab\ntrain_sentences = [item['sentence'] for item in train_data]\nV = []\nfor t in train_sentences:\n    tokenized_sentence = ViTokenizer.tokenize(t)\n    V = V + tokenized_sentence.split()\n    \nV = list(set(V))","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:51:03.598205Z","iopub.execute_input":"2025-11-28T01:51:03.598603Z","iopub.status.idle":"2025-11-28T01:51:09.929766Z","shell.execute_reply.started":"2025-11-28T01:51:03.598585Z","shell.execute_reply":"2025-11-28T01:51:09.929205Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Create label mapping ","metadata":{}},{"cell_type":"code","source":"# Word to index\nword2idx = {word: idx for idx, word in enumerate(V)}\nword2idx['PAD'] = 0\nword2idx['UNK'] = 1\n\n# Index to word\nidx2word = {idx: word for word, idx in word2idx.items()}\n\n# Label to index\nunique_labels = sorted(list(set([item['topic'] for item in train_data])))\nlabel2idx = {label: idx for idx, label in enumerate(unique_labels)}\nidx2label = {idx: label for label, idx in label2idx.items()}","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:51:09.930388Z","iopub.execute_input":"2025-11-28T01:51:09.930631Z","iopub.status.idle":"2025-11-28T01:51:09.937575Z","shell.execute_reply.started":"2025-11-28T01:51:09.930607Z","shell.execute_reply":"2025-11-28T01:51:09.936911Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Encoding sentences","metadata":{}},{"cell_type":"code","source":"# ============================\n# DATASET + DATALOADER\n# ============================\nclass TopicDataset(Dataset):\n    def __init__(self, encoded_sentences, labels):\n        self.encoded_sentences = encoded_sentences\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.encoded_sentences)\n    \n    def __getitem__(self, idx):\n        return torch.tensor(self.encoded_sentences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n\n# Encode function\ndef encode_sentence(sentence, max_length):\n    tokenized = ViTokenizer.tokenize(sentence).split()\n    indices = [word2idx.get(token, word2idx['UNK']) for token in tokenized]\n    \n    if max_length:\n        if len(indices) > max_length:\n            indices = indices[:max_length]\n        else:\n            indices = indices + [word2idx['PAD']] * (max_length - len(indices))\n    \n    return indices\n\n# Encode sentences\ntrain_encoded = [encode_sentence(item['sentence'], max_length=256) for item in train_data]\ntrain_labels = [label2idx[item['topic']] for item in train_data]\n\ndev_encoded = [encode_sentence(item['sentence'], max_length=256) for item in dev_data]\ndev_labels = [label2idx[item['topic']] for item in dev_data]\n\ntest_encoded = [encode_sentence(item['sentence'], max_length=256) for item in test_data]\ntest_labels = [label2idx[item['topic']] for item in test_data]\n\n# Create DataLoaders\ntrain_dataset = TopicDataset(train_encoded, train_labels)\ndev_dataset = TopicDataset(dev_encoded, dev_labels)\ntest_dataset = TopicDataset(test_encoded, test_labels)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:51:09.938457Z","iopub.execute_input":"2025-11-28T01:51:09.938789Z","iopub.status.idle":"2025-11-28T01:51:12.864976Z","shell.execute_reply.started":"2025-11-28T01:51:09.938765Z","shell.execute_reply":"2025-11-28T01:51:12.864359Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# ============================\n# LOAD PRETRAINED W2V_NER.VEC\n# ============================\nembedding_dim = 300\nembeddings_index = {}\n\nwith open(\"/kaggle/input/embedding-model/FastText_ner.vec\", \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        values = line.rstrip().split(\" \")\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = vector\n\n\n# ============================\n# BUILD EMBEDDING MATRIX\n# ============================\nembedding_matrix = np.zeros((len(word2idx), embedding_dim))\n\nfor word, idx in word2idx.items():\n    vector = embeddings_index.get(word)\n    if vector is not None:\n        embedding_matrix[idx] = vector\n    else:\n        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n\nembedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:51:12.865697Z","iopub.execute_input":"2025-11-28T01:51:12.865913Z","iopub.status.idle":"2025-11-28T01:51:15.328558Z","shell.execute_reply.started":"2025-11-28T01:51:12.865897Z","shell.execute_reply":"2025-11-28T01:51:15.327972Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============================\n# MODEL WITH PRETRAINED EMBEDDING\n# ============================\nclass Classifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes, embedding_matrix):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.embedding.weight = nn.Parameter(embedding_matrix, requires_grad=True)\n\n        self.output = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        embeds = self.embedding(x) # [batch, seq_len, embed_dim]\n        doc_vec = embeds.mean(dim=1) # Mean-pooling\n        logits = self.output(doc_vec) \n        return logits\n\nmodel = Classifier(vocab_size=len(word2idx), embed_dim=embedding_dim, num_classes=len(idx2label), embedding_matrix=embedding_matrix)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:51:15.329287Z","iopub.execute_input":"2025-11-28T01:51:15.329555Z","iopub.status.idle":"2025-11-28T01:51:15.502261Z","shell.execute_reply.started":"2025-11-28T01:51:15.329525Z","shell.execute_reply":"2025-11-28T01:51:15.501443Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Classifier(\n  (embedding): Embedding(3706, 300)\n  (output): Linear(in_features=300, out_features=4, bias=True)\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Train setup","metadata":{}},{"cell_type":"code","source":"# ============================\n# TRAINING SETUP\n# ============================\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nepochs = 20\n\n\n# ============================\n# TRAIN LOOP\n# ============================\ndef evaluate(loader):\n    model.eval()\n    preds, trues = [], []\n    with torch.no_grad():\n        for X, y in loader:\n            X, y = X.to(device), y.to(device)\n            logits = model(X)\n            pred = logits.argmax(dim=1)\n            preds.extend(pred.cpu().tolist())\n            trues.extend(y.cpu().tolist())\n    return (\n        accuracy_score(trues, preds),\n        f1_score(trues, preds, average=\"macro\"),\n        precision_score(trues, preds, average=\"macro\"),\n        recall_score(trues, preds, average=\"macro\"),\n    )","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:51:15.503130Z","iopub.execute_input":"2025-11-28T01:51:15.503458Z","iopub.status.idle":"2025-11-28T01:51:18.043470Z","shell.execute_reply.started":"2025-11-28T01:51:15.503435Z","shell.execute_reply":"2025-11-28T01:51:18.042885Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"best_f1 = 0.0\nbest_model_state = None\n\nfor epoch in range(epochs):\n    model.train()\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n\n    for X, y in train_bar:\n        X, y = X.to(device), y.to(device)\n\n        optimizer.zero_grad()\n        logits = model(X)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        train_bar.set_postfix(loss=loss.item())\n\n    # Evaluate on dev set\n    acc, f1, prec, rec = evaluate(dev_loader)\n    print(f\"Epoch {epoch+1}/{epochs} — Dev Acc: {acc:.4f}, F1: {f1:.4f}\")\n\n    # Save best model\n    if f1 > best_f1:\n        best_f1 = f1\n        best_model_state = model.state_dict()\n        torch.save(best_model_state, \"best_model.pth\")\n        print(f\"*** Best model updated at epoch {epoch+1} with F1: {best_f1:.4f} ***\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T01:51:18.045112Z","iopub.execute_input":"2025-11-28T01:51:18.045388Z","iopub.status.idle":"2025-11-28T01:51:49.504660Z","shell.execute_reply.started":"2025-11-28T01:51:18.045373Z","shell.execute_reply":"2025-11-28T01:51:49.503970Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 — Dev Acc: 0.7997, F1: 0.3667\n*** Best model updated at epoch 1 with F1: 0.3667 ***\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 — Dev Acc: 0.8427, F1: 0.6164\n*** Best model updated at epoch 2 with F1: 0.6164 ***\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 — Dev Acc: 0.8648, F1: 0.7463\n*** Best model updated at epoch 3 with F1: 0.7463 ***\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 — Dev Acc: 0.8617, F1: 0.7402\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 — Dev Acc: 0.8288, F1: 0.6113\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 — Dev Acc: 0.8282, F1: 0.7167\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 — Dev Acc: 0.8351, F1: 0.6653\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 — Dev Acc: 0.7928, F1: 0.6189\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 — Dev Acc: 0.8484, F1: 0.7186\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 — Dev Acc: 0.8219, F1: 0.7096\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 — Dev Acc: 0.8080, F1: 0.6667\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 — Dev Acc: 0.8389, F1: 0.7050\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 — Dev Acc: 0.8238, F1: 0.6601\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 — Dev Acc: 0.8061, F1: 0.6754\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 — Dev Acc: 0.8332, F1: 0.6878\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 — Dev Acc: 0.8421, F1: 0.6825\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 — Dev Acc: 0.8326, F1: 0.6778\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 — Dev Acc: 0.8269, F1: 0.6537\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 — Dev Acc: 0.8187, F1: 0.6547\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 — Dev Acc: 0.8320, F1: 0.6583\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# ============================\n#  TEST\n# ============================\n\n# Load best model\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\n\n# Evaluate on test set\nacc, f1, prec, rec = evaluate(test_loader)\nprint(\"\\n=== TEST RESULT ===\")\nprint(f\"Accuracy : {acc:.4f}\")\nprint(f\"F1       : {f1:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall   : {rec:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T01:51:49.505350Z","iopub.execute_input":"2025-11-28T01:51:49.505530Z","iopub.status.idle":"2025-11-28T01:51:49.663319Z","shell.execute_reply.started":"2025-11-28T01:51:49.505516Z","shell.execute_reply":"2025-11-28T01:51:49.662580Z"}},"outputs":[{"name":"stdout","text":"\n=== TEST RESULT ===\nAccuracy : 0.8557\nF1       : 0.7375\nPrecision: 0.7624\nRecall   : 0.7200\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}